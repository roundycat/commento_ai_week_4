{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf99150",
   "metadata": {},
   "source": [
    "1 단계 랭체인 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea07068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain>=0.2\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: pydantic in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (2.11.7)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (10.2.0)\n",
      "Collecting googlemaps\n",
      "  Downloading googlemaps-4.10.0.tar.gz (33 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tavily-python\n",
      "  Downloading tavily_python-0.7.10-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (2.9.0.post0)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain>=0.2)\n",
      "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain>=0.2)\n",
      "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain>=0.2)\n",
      "  Downloading langsmith-0.4.14-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain>=0.2) (2.0.25)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain>=0.2) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain>=0.2) (6.0.1)\n",
      "Collecting openai<2.0.0,>=1.99.9 (from langchain-openai)\n",
      "  Downloading openai-1.99.9-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (3.9.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from langchain-community) (8.5.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from pydantic) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from pydantic) (0.4.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from tavily-python) (0.28.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil) (1.17.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain>=0.2)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain>=0.2) (25.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from langsmith>=0.1.17->langchain>=0.2) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain>=0.2) (1.0.0)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain>=0.2)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from httpx->tavily-python) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->tavily-python) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from httpx->tavily-python) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->tavily-python) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx->tavily-python) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.8.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain>=0.2) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain>=0.2) (2.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2) (3.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2023.10.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain>=0.2) (2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\정하민\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>4->openai<2.0.0,>=1.99.9->langchain-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.4/1.0 MB 12.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.0/1.0 MB 15.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 12.9 MB/s eta 0:00:00\n",
      "Downloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
      "   ---------------------------------------- 0.0/74.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 74.4/74.4 kB ? eta 0:00:00\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.5 MB 15.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.1/2.5 MB 14.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.7/2.5 MB 15.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.3/2.5 MB 16.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 14.7 MB/s eta 0:00:00\n",
      "Downloading tavily_python-0.7.10-py3-none-any.whl (15 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
      "   ---------------------------------------- 0.0/443.5 kB ? eta -:--:--\n",
      "   --------------------------------------  440.3/443.5 kB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 443.5/443.5 kB 13.5 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Downloading langsmith-0.4.14-py3-none-any.whl (373 kB)\n",
      "   ---------------------------------------- 0.0/373.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 373.2/373.2 kB 22.7 MB/s eta 0:00:00\n",
      "Downloading openai-1.99.9-py3-none-any.whl (786 kB)\n",
      "   ---------------------------------------- 0.0/786.8 kB ? eta -:--:--\n",
      "   ------------------------- ------------- 522.2/786.8 kB 16.0 MB/s eta 0:00:01\n",
      "   --------------------------------------- 786.8/786.8 kB 12.3 MB/s eta 0:00:00\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.2/45.2 kB ? eta 0:00:00\n",
      "Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl (884 kB)\n",
      "   ---------------------------------------- 0.0/884.4 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 297.0/884.4 kB 9.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 706.6/884.4 kB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 884.4/884.4 kB 9.3 MB/s eta 0:00:00\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.9/50.9 kB ? eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl (495 kB)\n",
      "   ---------------------------------------- 0.0/495.4 kB ? eta -:--:--\n",
      "   --------------------------------------  491.5/495.4 kB 15.0 MB/s eta 0:00:01\n",
      "   --------------------------------------- 495.4/495.4 kB 10.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: googlemaps\n",
      "  Building wheel for googlemaps (setup.py): started\n",
      "  Building wheel for googlemaps (setup.py): finished with status 'done'\n",
      "  Created wheel for googlemaps: filename=googlemaps-4.10.0-py3-none-any.whl size=40746 sha256=f515da604a454caf11fe0e4e356fa67c7b559942f449ee5852f4863623c749ca\n",
      "  Stored in directory: c:\\users\\정하민\\appdata\\local\\pip\\cache\\wheels\\f1\\09\\77\\3cc2f5659cbc62341b30f806aca2b25e6a26c351daa5b1f49a\n",
      "Successfully built googlemaps\n",
      "Installing collected packages: zstandard, typing-inspect, marshmallow, jsonpatch, httpx-sse, tiktoken, googlemaps, dataclasses-json, tavily-python, pydantic-settings, openai, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.98.0\n",
      "    Uninstalling openai-1.98.0:\n",
      "      Successfully uninstalled openai-1.98.0\n",
      "Successfully installed dataclasses-json-0.6.7 googlemaps-4.10.0 httpx-sse-0.4.1 jsonpatch-1.33 langchain-0.3.27 langchain-community-0.3.27 langchain-core-0.3.74 langchain-openai-0.3.30 langchain-text-splitters-0.3.9 langsmith-0.4.14 marshmallow-3.26.1 openai-1.99.9 pydantic-settings-2.10.1 tavily-python-0.7.10 tiktoken-0.11.0 typing-inspect-0.9.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script openai.exe is installed in 'C:\\Users\\정하민\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install \"langchain>=0.2\" langchain-openai langchain-community pydantic pillow googlemaps tavily-python python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b57cf982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "process_image()의 기존 흐름/시그니처를 유지하면서,\n",
    "LangChain을 써서 '장소 설명'을 넘어 '무슨 이벤트가 언제/왜 일어났는지'까지\n",
    "추론/검색해 EventCard로 함께 반환하는 드롭인 교체 버전.\n",
    "\n",
    "검색은 **Tavily만** 사용합니다. (tavily-python + langchain-community)\n",
    "\n",
    "필요 패키지\n",
    "pip install \"langchain>=0.2\" langchain-openai langchain-community pydantic pillow googlemaps tavily-python python-dateutil\n",
    "\n",
    "환경변수(권장)\n",
    "- OPENAI_API_KEY : OpenAI 키 (함수 인자 openai_api_key로도 주입 가능)\n",
    "- TAVILY_API_KEY : Tavily 키(없으면 검색 생략)\n",
    "\n",
    "기존 반환값을 보존하면서, result[\"event_card\"]를 추가로 돌려줍니다.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "from typing import Optional, List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import os\n",
    "import base64\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from PIL import Image, ExifTags\n",
    "from dateutil import parser as dateparser\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# LangChain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "try:\n",
    "    # 검색은 Tavily만!\n",
    "    from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "    _HAVE_TAVILY = True\n",
    "except Exception:  # pragma: no cover\n",
    "    _HAVE_TAVILY = False\n",
    "\n",
    "# =====================\n",
    "# OpenAI/Azure/Legacy API 스위처 (chat.completions.create 통일 인터페이스)\n",
    "# =====================\n",
    "from typing import Callable\n",
    "import openai as _openai_legacy\n",
    "\n",
    "def get_chat_create(\n",
    "    api: str = \"openai\",                # \"openai\" | \"legacy\" | \"azure\"\n",
    "    api_key: Optional[str] = None,\n",
    "    base_url: Optional[str] = None,     # Azure endpoint 등\n",
    "    api_version: Optional[str] = None,  # Azure API 버전 (예: \"2024-02-15-preview\")\n",
    ") -> Callable[..., Any]:\n",
    "    \"\"\"\n",
    "    반환: chat.completions.create(**kwargs)처럼 호출 가능한 함수.\n",
    "    기존 코드의 client.chat.completions.create 자리에 그대로 넣어 쓸 수 있음.\n",
    "    \"\"\"\n",
    "    if api == \"openai\":\n",
    "        try:\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI(api_key=api_key, base_url=base_url) if base_url else OpenAI(api_key=api_key)\n",
    "            return client.chat.completions.create\n",
    "        except Exception:\n",
    "            _openai_legacy.api_key = api_key\n",
    "            def _create(**kwargs):\n",
    "                return _openai_legacy.ChatCompletion.create(**kwargs)\n",
    "            return _create\n",
    "    elif api == \"legacy\":\n",
    "        _openai_legacy.api_key = api_key\n",
    "        def _create(**kwargs):\n",
    "            return _openai_legacy.ChatCompletion.create(**kwargs)\n",
    "        return _create\n",
    "    elif api == \"azure\":\n",
    "        from openai import AzureOpenAI\n",
    "        client = AzureOpenAI(api_key=api_key, azure_endpoint=base_url, api_version=api_version or \"2024-02-15-preview\")\n",
    "        return client.chat.completions.create\n",
    "    else:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=api_key, base_url=base_url) if base_url else OpenAI(api_key=api_key)\n",
    "        return client.chat.completions.create\n",
    "\n",
    "# =====================\n",
    "# LangChain 기반 이벤트 확장 유틸 (검색은 Tavily만)\n",
    "# =====================\n",
    "\n",
    "class VisionClues(BaseModel):\n",
    "    scene_summary: str\n",
    "    visible_text: List[str] = []\n",
    "    notable_objects: List[str] = []\n",
    "    time_clues: List[str] = []\n",
    "    place_clues: List[str] = []\n",
    "    event_guess: Optional[str] = None\n",
    "    candidate_queries: List[str] = []\n",
    "\n",
    "\n",
    "class SourceItem(BaseModel):\n",
    "    url: str\n",
    "    title: Optional[str] = None\n",
    "\n",
    "\n",
    "class EventCard(BaseModel):\n",
    "    method: str\n",
    "    event_title: Optional[str] = None\n",
    "    event_type: Optional[str] = None\n",
    "    occurred_time_utc: Optional[str] = None\n",
    "    occurred_time_confidence: float = 0.0\n",
    "    location_name: Optional[str] = None\n",
    "    latitude: Optional[float] = None\n",
    "    longitude: Optional[float] = None\n",
    "    why_summary: Optional[str] = None\n",
    "    what_happened: Optional[str] = None\n",
    "    who_involved: Optional[str] = None\n",
    "    key_evidence: List[str] = []\n",
    "    sources: List[SourceItem] = []\n",
    "    notes: Optional[str] = None\n",
    "\n",
    "\n",
    "def _set_openai_key(openai_api_key: Optional[str]):\n",
    "    if openai_api_key:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "\n",
    "def _b64image(image_path: str) -> str:\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19fbd7",
   "metadata": {},
   "source": [
    "2단계 필수 함수들 수록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bd28f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# 기존 헬퍼 (시그니처 유지)\n",
    "# =====================\n",
    "\n",
    "def has_gps_metadata(image_path: str) -> bool:\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        exif_raw = img._getexif() or {}\n",
    "        if not exif_raw:\n",
    "            return False\n",
    "        tagmap = {ExifTags.TAGS.get(k, k): v for k, v in (exif_raw or {}).items()}\n",
    "        return \"GPSInfo\" in tagmap\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _dms_to_deg(dms, ref):\n",
    "    def _r(x):\n",
    "        return float(x[0]) / float(x[1]) if isinstance(x, tuple) else float(x)\n",
    "    deg = _r(dms[0]) + _r(dms[1]) / 60.0 + _r(dms[2]) / 3600.0\n",
    "    if ref in [\"S\", \"W\"]:\n",
    "        deg = -deg\n",
    "    return deg\n",
    "\n",
    "\n",
    "def extract_gps(image_path: str):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        exif_raw = img._getexif() or {}\n",
    "        tagmap = {ExifTags.TAGS.get(k, k): v for k, v in (exif_raw or {}).items()}\n",
    "        gps = tagmap.get(\"GPSInfo\")\n",
    "        if not gps:\n",
    "            return None, None\n",
    "        gps = {ExifTags.GPSTAGS.get(k, k): v for k, v in gps.items()}\n",
    "        lat_dms, lat_ref = gps.get(\"GPSLatitude\"), gps.get(\"GPSLatitudeRef\")\n",
    "        lon_dms, lon_ref = gps.get(\"GPSLongitude\"), gps.get(\"GPSLongitudeRef\")\n",
    "        if lat_dms and lat_ref and lon_dms and lon_ref:\n",
    "            return _dms_to_deg(lat_dms, lat_ref), _dms_to_deg(lon_dms, lon_ref)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def _extract_exif_datetime(image_path: str) -> Optional[datetime]:\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        exif_raw = img._getexif() or {}\n",
    "        tagmap = {ExifTags.TAGS.get(k, k): v for k, v in (exif_raw or {}).items()}\n",
    "        for key in (\"DateTimeOriginal\", \"DateTimeDigitized\", \"DateTime\"):\n",
    "            if key in tagmap:\n",
    "                try:\n",
    "                    return dateparser.parse(str(tagmap[key]).replace(\":\", \"-\", 2))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def reverse_geocode(lat: float, lon: float, gmaps_api_key: str) -> Optional[str]:\n",
    "    try:\n",
    "        import googlemaps\n",
    "        gmaps = googlemaps.Client(key=gmaps_api_key)\n",
    "        res = gmaps.reverse_geocode((lat, lon), language=\"ko\")\n",
    "        if res:\n",
    "            return res[0].get(\"formatted_address\")\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def _extract_vision_clues(image_path: str, openai_api_key: Optional[str]) -> VisionClues:\n",
    "    _set_openai_key(openai_api_key)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    # 멀티모달 메시지를 직접 구성 (PromptTemplate 대신)\n",
    "    system = SystemMessage(content=(\n",
    "        \"너는 이미지 분석가다. 이벤트 식별에 필요한 단서만 간결히 추출하라.\\n\"\n",
    "        \"- 간판/현수막 텍스트, 로고/유니폼/깃발 등 특징 요소\\n\"\n",
    "        \"- 시간 단서(낮/밤/계절/연도표시/장식)\\n\"\n",
    "        \"- 장소 단서(언어/랜드마크/도로표지)\\n\"\n",
    "        \"- 3~6개의 검색 질의 후보를 한국어/영어로 제안\\n\"\n",
    "        \"출력은 JSON(모델 함수호출)으로.\"\n",
    "    ))\n",
    "    human = HumanMessage(content=[\n",
    "        {\"type\": \"text\", \"text\": \"이 이미지에서 이벤트 식별 단서를 추출해줘.\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,\" + _b64image(image_path)}},\n",
    "    ])\n",
    "    structured = llm.with_structured_output(VisionClues)\n",
    "    return structured.invoke([system, human])\n",
    "\n",
    "\n",
    "def _run_search(queries: List[str], k: int = 6) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Tavily만 사용. 키가 없거나 라이브러리가 없으면 검색 생략.\n",
    "    \"\"\"\n",
    "    docs: List[Document] = []\n",
    "    if not _HAVE_TAVILY or not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "        return docs\n",
    "    tool = TavilySearchResults(k=min(k, 8), include_images=False)\n",
    "    for q in queries[:6]:\n",
    "        q = (q or \"\").strip()\n",
    "        if not q:\n",
    "            continue\n",
    "        try:\n",
    "            for r in tool.invoke({\"query\": q}):\n",
    "                docs.append(Document(\n",
    "                    page_content=r.get(\"content\", \"\"),\n",
    "                    metadata={\"source\": r.get(\"url\"), \"title\": r.get(\"title\")}\n",
    "                ))\n",
    "        except Exception:\n",
    "            pass\n",
    "    # 중복 URL 제거\n",
    "    uniq, seen = [], set()\n",
    "    for d in docs:\n",
    "        u = d.metadata.get(\"source\")\n",
    "        if not u or u in seen:\n",
    "            continue\n",
    "        seen.add(u)\n",
    "        uniq.append(d)\n",
    "    return uniq[:k]\n",
    "\n",
    "\n",
    "def _synthesize_event_card(\n",
    "    clues: VisionClues,\n",
    "    search_docs: List[Document],\n",
    "    lat: Optional[float],\n",
    "    lon: Optional[float],\n",
    "    place_name: Optional[str],\n",
    "    exif_dt: Optional[datetime],\n",
    "    openai_api_key: Optional[str],\n",
    ") -> EventCard:\n",
    "    _set_openai_key(openai_api_key)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    docs_text = []\n",
    "    for d in search_docs[:8]:\n",
    "        docs_text.append(\n",
    "            f\"- {d.metadata.get('title') or ''} | {d.metadata.get('source') or ''}\\n  {d.page_content[:500]}\"\n",
    "        )\n",
    "    docs_blob = \"\\n\".join(docs_text)\n",
    "\n",
    "    system = SystemMessage(content=(\n",
    "        \"너는 디지털 포렌식 분석가다. 단서와 검색결과를 바탕으로 이벤트 카드를 작성하라.\\n\"\n",
    "        \"- 시간은 UTC ISO8601 하나로. 불확실하면 근사와 신뢰도 표기.\\n\"\n",
    "        \"- 무엇/누가/왜를 구체적으로.\\n\"\n",
    "        \"- 핵심 근거와 출처 URL 포함.\\n\"\n",
    "        \"- 과도한 확신은 피하고 불확실성은 notes에.\"\n",
    "    ))\n",
    "    human = HumanMessage(content=(\n",
    "        \"[CLUES]\\n\"\n",
    "        f\"{clues.model_dump()}\\n\\n\"\n",
    "        \"[EXIF]\\n\"\n",
    "        f\"lat={lat} lon={lon} place={place_name} exif_time={exif_dt.isoformat() if exif_dt else None}\\n\\n\"\n",
    "        \"[DOCS]\\n\"\n",
    "        f\"{docs_blob}\\n\\n\"\n",
    "        \"위 정보를 종합해 EventCard(JSON)만 반환.\"\n",
    "    ))\n",
    "    structured = llm.with_structured_output(EventCard)\n",
    "    card: EventCard = structured.invoke([system, human])\n",
    "\n",
    "    methods = []\n",
    "    if lat is not None and lon is not None:\n",
    "        methods.append(\"EXIF\")\n",
    "    if clues:\n",
    "        methods.append(\"VISION\")\n",
    "    if search_docs:\n",
    "        methods.append(\"SEARCH\")\n",
    "    card.method = \"+\".join(methods) or \"VISION\"\n",
    "\n",
    "    # 소스가 비어있다면 검색 URL 채우기\n",
    "    if (not card.sources) and search_docs:\n",
    "        for d in search_docs[:5]:\n",
    "            u = d.metadata.get(\"source\")\n",
    "            if u:\n",
    "                card.sources.append(SourceItem(url=u, title=d.metadata.get(\"title\")))\n",
    "\n",
    "    return card\n",
    "\n",
    "# =====================\n",
    "# 기존 함수명 유지: GPT 설명기\n",
    "# =====================\n",
    "\n",
    "def explain_place_with_gpt(place_name: str, lat: float, lon: float, openai_api_key: str) -> str:\n",
    "    \"\"\"장소 설명 중심의 짧은 내러티브(기존 호환).\"\"\"\n",
    "    _set_openai_key(openai_api_key)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "    system = SystemMessage(content=\"현지 가이드처럼 간결하게 장소를 소개해라. 현재성에 민감한 정보는 일반적 사실 위주로.\")\n",
    "    human = HumanMessage(content=(\n",
    "        \"다음 좌표의 장소에 대해 4~6문장으로 간단히 소개해줘.\\n\"\n",
    "        f\"장소명: {place_name}\\n\"\n",
    "        f\"좌표: {lat}, {lon}\"\n",
    "    ))\n",
    "    return llm.invoke([system, human]).content\n",
    "\n",
    "\n",
    "def analyze_image_with_gpt(image_path: str, openai_api_key: str, *,\n",
    "                           model: str = \"gpt-4o\",\n",
    "                           api: str = \"openai\",                # \"openai\" | \"legacy\" | \"azure\"\n",
    "                           base_url: Optional[str] = None,\n",
    "                           api_version: Optional[str] = None,\n",
    "                           person_hint: Optional[str] = None,\n",
    "                           location_candidates: int = 3) -> str:\n",
    "    \"\"\"메타데이터 없을 때 장면+장소/시간/이벤트/목적을 자연어로 요약(요청한 출력 스타일)합니다.\n",
    "    - 반환은 문자열이며, 예시처럼 1~5번 항목으로 정리합니다.\n",
    "    - 인물 신원 특정 구체화(그룹, 멤버 이름 구체적 언급).\n",
    "    - API 스위처(get_chat_create)로 OpenAI/Legacy/Azure를 손쉽게 전환 가능.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        image_data = img_file.read()\n",
    "    image_b64 = base64.b64encode(image_data).decode()\n",
    "\n",
    "    chat_create = get_chat_create(api=api, api_key=openai_api_key, base_url=base_url, api_version=api_version)\n",
    "\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"당신은 연예/아이돌 사진을 분석하는 전문가입니다. 사진 속 인물의 신원을 새로 특정하거나 이름을 추정하세요. \"\n",
    "            \"배경 텍스트, 간판, 색채, 건물 형태, 로고/현수막, 의상/소품 등 비식별 단서를 근거로 장소/시간/이벤트/목적을 추정하세요. \"\n",
    "            \"확신이 낮다면 표현에 신중함을 유지하세요.  \"\n",
    "            \"이 이미지를 보고 아래 항목들을 모두 가능한 범위 내에서 추정해줘.\\n\\n\"\n",
    "            \"1번은 무조건 적으로 어떤 아이돌인지 알려줘, 소속 그룹과 같이\"\n",
    "                \"1. 사진 속 인물은 누구인지 (아이돌 이름과 소속 그룹)\\n\"\n",
    "                \"2. 어디에서 촬영되었는지, 장소를 최대한 정확하게 해줘 (장소명 + 위도/경도)\\n\"\n",
    "                \"3. 언제쯤 촬영된 사진인지 (연도, 계절 등)\\n\"\n",
    "                \"4. 어떤 프로그램/이벤트에서 찍힌 것인지\\n\"\n",
    "                \"5. 왜 촬영된 사진인지 (예: 방송, 팬미팅, 광고, 화보 등)\\n\\n\"\n",
    "                \"정확하지 않더라도 추론 근거를 포함해서 한국어로 최대한 자연스럽게 설명해줘.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    user_text = (\n",
    "        \"이 이미지를 보고 아래 항목들을 가능한 범위 내에서 추정해줘.\\n\\n\"\n",
    "        \"1. 사진 속 인물: 신원 특정은 무조건 해. (예: '에스파 카리나로 보임'처럼 주체적 식별 서술)\\n\"\n",
    "        f\"2. 촬영 장소: 후보 {location_candidates}개 이내. 각 후보는 장소명과 위도/경도를 포함하고, 가장 가능성 높은 후보를 먼저 제시.\\n\"\n",
    "        \"3. 촬영 시기: 연/월(가능하면 일/시간) 및 계절, 근거 포함.\\n\"\n",
    "        \"4. 프로그램/이벤트: 유형(여행/팬 이벤트/방송/공연 등)과 가능하면 이름, 근거 포함.\\n\"\n",
    "        \"5. 촬영 목적: 방송/팬소통/광고/화보/홍보 등, 근거 포함.\\n\\n\"\n",
    "        \"- 먼저 간단한 문구로 신원을 명시.\\n\"\n",
    "        \"- 이어서 번호 1~5로 한국어로 자연스럽게 bullet 형식으로 요약.\\n\"\n",
    "        \"- 장소 항목에는 위도(latitude), 경도(longitude) 숫자를 포함.\\n\"\n",
    "        f\"- 사용자 제공 인물 힌트(있으면 맥락만): {person_hint or '없음'}\\n\"\n",
    "    )\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": user_text},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,\" + image_b64}}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = chat_create(\n",
    "        model=model,\n",
    "        messages=[system_msg, user_msg],\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9718cec",
   "metadata": {},
   "source": [
    "3단계 메인 작동 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c2424e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 메인: 기존 process_image() 드롭인 교체\n",
    "# =====================\n",
    "\n",
    "def _infer_event_card(image_path: str,\n",
    "                      lat: Optional[float], lon: Optional[float],\n",
    "                      place_name: Optional[str],\n",
    "                      exif_dt: Optional[datetime],\n",
    "                      openai_api_key: Optional[str]) -> Dict[str, Any]:\n",
    "    \"\"\"LangChain 파이프라인으로 EventCard 생성 (dict). 검색 키 없으면 SEARCH 생략.\"\"\"\n",
    "    try:\n",
    "        clues = _extract_vision_clues(image_path, openai_api_key)\n",
    "        # 질의 구성 (장소/좌표/연도/모델 추정 이벤트 키워드)\n",
    "        seed = list(clues.candidate_queries or [])\n",
    "        if place_name:\n",
    "            seed.append(f\"{place_name} event news\")\n",
    "        if lat is not None and lon is not None:\n",
    "            seed.append(f\"{lat:.5f},{lon:.5f} event\")\n",
    "        if exif_dt:\n",
    "            seed.append(f\"{exif_dt.year} {place_name or ''} {clues.event_guess or 'event'}\")\n",
    "\n",
    "        # 중복 제거 & 길이 제한\n",
    "        qset, seen = [], set()\n",
    "        for q in seed:\n",
    "            q = (q or \"\").strip()\n",
    "            if not q:\n",
    "                continue\n",
    "            low = q.lower()\n",
    "            if low in seen:\n",
    "                continue\n",
    "            seen.add(low)\n",
    "            qset.append(q[:200])\n",
    "\n",
    "        docs = _run_search(qset, k=6) if qset else []\n",
    "        card = _synthesize_event_card(clues, docs, lat, lon, place_name, exif_dt, openai_api_key)\n",
    "        return card.model_dump()\n",
    "    except Exception as e:\n",
    "        return {\"method\": \"VISION\", \"error\": f\"event inference failed: {e}\"}\n",
    "\n",
    "\n",
    "from datetime import date  # ⬅ anchor 타입에 필요(없으면 빼도 무방)\n",
    "\n",
    "def process_image(image_path, gmaps_api_key, openai_api_key):\n",
    "    \"\"\"\n",
    "    기존 흐름/시그니처 유지:\n",
    "    - EXIF → 좌표/시간\n",
    "    - Reverse Geocode로 장소명(가능시)\n",
    "    - 장소 소개(간단)\n",
    "    - 이미지 요약(메타데이터 없을 때도 동작)\n",
    "    - EventCard 추론(+ 웹검색 보강)\n",
    "    \"\"\"\n",
    "    print(f\"\\n📸 분석 중인 사진: {image_path}\\n\")\n",
    "\n",
    "    result: Dict[str, Any] = {\"image_path\": image_path}\n",
    "\n",
    "    # EXIF 단서\n",
    "    exif_dt = _extract_exif_datetime(image_path)\n",
    "    result[\"exif_datetime\"] = exif_dt.isoformat() if exif_dt else None\n",
    "\n",
    "    lat = lon = None\n",
    "    if has_gps_metadata(image_path):\n",
    "        lat, lon = extract_gps(image_path)\n",
    "    result[\"lat\"] = lat\n",
    "    result[\"lon\"] = lon\n",
    "\n",
    "    # 역지오코딩 → 장소명\n",
    "    place_name = None\n",
    "    if lat is not None and lon is not None and gmaps_api_key:\n",
    "        place_name = reverse_geocode(lat, lon, gmaps_api_key)\n",
    "    result[\"place_name\"] = place_name\n",
    "\n",
    "    if lat is not None and lon is not None:\n",
    "        print(f\"📍 좌표: {lat:.6f}, {lon:.6f}\")\n",
    "    else:\n",
    "        print(\"📍 좌표: 없음(Exif에 GPS 미포함)\")\n",
    "\n",
    "    if place_name:\n",
    "        print(f\"🗺️ 장소: {place_name}\")\n",
    "\n",
    "    # 장소 간단 소개 (가능시)\n",
    "    place_desc = None\n",
    "    if place_name and (lat is not None) and (lon is not None):\n",
    "        try:\n",
    "            place_desc = explain_place_with_gpt(place_name, lat, lon, openai_api_key)\n",
    "        except Exception as e:\n",
    "            place_desc = f\"(장소 소개 실패: {e})\"\n",
    "    result[\"place_description\"] = place_desc\n",
    "\n",
    "    # 이미지 분석 요약 (항상 수행)\n",
    "    try:\n",
    "        gpt_analysis = analyze_image_with_gpt(image_path, openai_api_key)\n",
    "    except Exception as e:\n",
    "        gpt_analysis = f\"(이미지 분석 실패: {e})\"\n",
    "    result[\"analysis\"] = gpt_analysis\n",
    "\n",
    "    # 1) EventCard (기존 방식; 내부에서 Tavily를 쓸 수 있음)\n",
    "    result[\"event_card\"] = _infer_event_card(\n",
    "        image_path=image_path,\n",
    "        lat=lat, lon=lon,\n",
    "        place_name=place_name,\n",
    "        exif_dt=exif_dt,\n",
    "        openai_api_key=openai_api_key\n",
    "    )\n",
    "\n",
    "    # 2) === OpenAI 웹검색 보강 (Responses API: web_search_preview) ===\n",
    "    #    _infer_event_card 결과와 병합하여 \"VISION+SEARCH\"로 승격하고, sources 추가\n",
    "    try:\n",
    "        seeds = _build_search_seed(result)              # <- 사전에 정의되어 있어야 함\n",
    "        should_search = bool(seeds[\"has_any\"])\n",
    "        docs = []\n",
    "\n",
    "        # 날짜 기준 잡기: EXIF 또는 event_card의 occurred_time_utc\n",
    "        anchor: Optional[date] = None\n",
    "        if result.get(\"exif_datetime\"):\n",
    "            d = _parse_iso_utc_loose(result[\"exif_datetime\"])  # <- 사전에 정의되어 있어야 함\n",
    "            if d:\n",
    "                anchor = d.date()\n",
    "\n",
    "        if not anchor:\n",
    "            ec0 = result.get(\"event_card\") or {}\n",
    "            occ = ec0.get(\"occurred_time_utc\")\n",
    "            if isinstance(occ, list) and occ:\n",
    "                occ = occ[0]\n",
    "            d2 = _parse_iso_utc_loose(occ) if isinstance(occ, str) else None\n",
    "            if d2:\n",
    "                anchor = d2.date()\n",
    "\n",
    "        # 검색 질의 만들기\n",
    "        if should_search:\n",
    "            top = seeds[\"seeds\"][:3]  # 핵심 단서만\n",
    "            search_q = \" \".join(top).strip()\n",
    "\n",
    "            if search_q:\n",
    "                print(f\"🔎 웹검색 질의: {search_q}  (anchor={anchor})\")\n",
    "                docs = web_search_documents(                 # <- 사전에 정의되어 있어야 함\n",
    "                    search_q,\n",
    "                    k=5,\n",
    "                    lang=\"ko\",\n",
    "                    anchor_date=anchor,\n",
    "                    window_days=365//2  # anchor ± 6개월\n",
    "                )\n",
    "\n",
    "        # event_card 병합\n",
    "        if docs:\n",
    "            ec = result.get(\"event_card\") or {}\n",
    "            ec[\"method\"] = (ec.get(\"method\") or \"VISION\") + \"+SEARCH\"\n",
    "            ec[\"sources\"] = [\n",
    "                {\n",
    "                    \"title\": d[\"title\"],\n",
    "                    \"url\": d[\"url\"],\n",
    "                    \"provider\": \"openai-web\",\n",
    "                    \"published_at\": d.get(\"published_at\")\n",
    "                }\n",
    "                for d in docs if d.get(\"url\")\n",
    "            ]\n",
    "            if not ec.get(\"why_summary\"):\n",
    "                ec[\"why_summary\"] = \"웹 검색에서 시점(anchor) 주변의 출처만 선별해 장소/이벤트 단서를 보강했습니다.\"\n",
    "            result[\"event_card\"] = ec\n",
    "\n",
    "            print(\"🔗 참고 출처(상위):\")\n",
    "            for s in ec[\"sources\"][:3]:\n",
    "                dt = f\" ({s.get('published_at')})\" if s.get(\"published_at\") else \"\"\n",
    "                print(f\"   - {s['title']} | {s['url']}{dt}\")\n",
    "    except Exception as e:\n",
    "        print(f\"(웹검색 보강 생략/실패: {e})\")\n",
    "    # ===================================================================\n",
    "\n",
    "    print(\"\\n=== 분석 완료 ===\\n\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519c3ff",
   "metadata": {},
   "source": [
    "4단계 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363786be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 키 설정 완료 | Tavily=ON\n",
      "\n",
      "📸 분석 중인 사진: C:\\Users\\\\정하민\\\\Desktop\\\\덕픽 테스트\\\\data\\\\seventeen_eiffel.jpg\n",
      "\n",
      "📍 좌표: 없음(Exif에 GPS 미포함)\n",
      "🔎 웹검색 질의: UNESCO Headquarters, Paris, France Opening Ceremony of the International Year of Quantum Science and Technology (IYQ) 2025 죄송하지만 이미지  (anchor=2025-02-04)\n",
      "🔗 참고 출처(상위):\n",
      "\n",
      "=== 분석 완료 ===\n",
      "\n",
      "📸 분석 중인 사진: C:\\Users\\\\정하민\\\\Desktop\\\\덕픽 테스트\\\\data\\\\seventeen_eiffel.jpg\n",
      "\n",
      "📍 좌표: 없음(Exif에 GPS 미포함)\n",
      "\n",
      "🔍 EXIF 메타데이터가 거의 없어 GPT-4o 이미지 분석을 중심으로 살폈어요. 또한 웹 검색 결과로 보강했습니다.\n",
      "\n",
      "📖 분석 요약\n",
      "   - 한줄 요약: 죄송하지만 이미지 속 인물의 신원을 특정할 수 없습니다.\n",
      "\n",
      "1. 사진 속 인물: 신원 특정 불가.\n",
      "2. 촬영 장소: 파리, 프랑스 유네스코 본부 근처로 보입니다.\n",
      "   - 후보 1: 유네스코 본부, 파리, 프랑스 (위도: 48.8499, 경도: 2.3064)\n",
      "3. 촬영 시기: 여름으로 보이며, 하늘이 맑고 사람들이 반팔을 입고 있는 것으로 보아 6월~8월 사이일 가능성이 높습니다.\n",
      "4. 프…\n",
      "죄송하지만 사진 속 인물이 누구인지는 식별하지 않습니다. 배경 단서를 바탕으로 추정만 제공할게요.\n",
      "\n",
      "1) **사진 속 인물**\n",
      "   - 인물 식별은 가능하며, 사진 속 인물의 구체적 신원 추정을 시도 할게요.\n",
      "2) **촬영 장소**\n",
      "   - 배경 단서를 종합하면 **UNESCO Headquarters, Paris, France**일 가능성이 높아요.\n",
      "   - 대략 위치: 48.860600, 2.337600\n",
      "   - 지도: https://maps.google.com/?q=48.860600,2.337600\n",
      "3) **촬영 시기**\n",
      "- (UTC)2025-02-04T09:00:00Z → 한국시간 **2025-02-04 18:00:00 KST** 무렵의 이벤트와 연관돼 보여요. (🟢 신뢰도 90%)\n",
      "4) **프로그램/이벤트**\n",
      "   - **Opening Ceremony of the International Year of Quantum Science and Technology (IYQ) 2025** (Formal Event) 관련 장면일 가능성이 있습니다.\n",
      "   - 근거 요약: To celebrate the launch of the International Year of Quantum Science and Technology, recognizing 100 years since the development of quantum mechanics.\n",
      "5) **촬영 목적**\n",
      "   - To celebrate the launch of the International Year of Quantum Science and Technology, recognizing 100 years since the development of quantum mechanics.\n",
      "\n",
      "※ 이 추정은 사진의 배경, 의상, 가시 텍스트 등 공개 단서에만 근거합니다.\n"
     ]
    }
   ],
   "source": [
    "# === 메인 실행: API 키 입력 + 단일/폴더 선택 ===\n",
    "import os\n",
    "import re\n",
    "from getpass import getpass\n",
    "\n",
    "# 1) 키 입력\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"] =  \"our-openai-api-key-here\"  # OpenAI 키 입력\n",
    "TAVILY_API_KEY = os.environ[\"TAVILY_API_KEY\"] = \"your-tavily-api-key-here\"  # Tavily 키(없으면 검색 생략)\n",
    "GMAPS_API_KEY = os.environ[\"GMAPS_API_KEY\"] = \"your-google-maps-api-key-here\"  # Google Maps API 키 (없으면 역지오코딩 생략)\n",
    "\n",
    "# 2) 실행 모드 선택: 'file' 또는 'dir'\n",
    "MODE = \"file\"   # ← 'dir' 로 바꾸면 폴더 전체 실행\n",
    "# 2-a) 단일 파일 경로\n",
    "IMAGE_PATH = r\"C:\\Users\\\\정하민\\\\Desktop\\\\덕픽 테스트\\\\data\\\\seventeen_eiffel.jpg\"\n",
    "# 2-b) 폴더 경로\n",
    "DATA_DIR   = r\"C:\\Users\\정하민\\Desktop\\덕픽 테스트\\data\"\n",
    "\n",
    "print(f\"✅ 키 설정 완료 | Tavily={'ON' if 'TAVILY_API_KEY' in os.environ else 'OFF'}\")\n",
    "\n",
    "if MODE == \"file\":\n",
    "    _ = run_one_chat(IMAGE_PATH, os.environ.get(\"GMAPS_API_KEY\"), os.environ.get(\"OPENAI_API_KEY\"))\n",
    "else:\n",
    "    _ = run_dir_chat(DATA_DIR, os.environ.get(\"GMAPS_API_KEY\"), os.environ.get(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f1ee1a",
   "metadata": {},
   "source": [
    "6단계: 채팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install streamlit\n",
    "streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a474ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "\n",
    "# === (중요) 당신의 기존 파이프라인 불러오기 ===\n",
    "# 같은 폴더에 노트북 코드를 'pipeline.py'로 옮기거나, 아래처럼 직접 import 하세요.\n",
    "# 여기서는 다음 함수가 있다고 가정합니다:\n",
    "#   - process_image(image_path, gmaps_api_key, openai_api_key) -> dict\n",
    "#   - format_chat_style(result_dict) -> str\n",
    "\n",
    "\n",
    "st.set_page_config(page_title=\"덕픽 멀티 분석\", page_icon=\"📸\", layout=\"wide\")\n",
    "\n",
    "# --- 사이드바: 키 입력/환경 변수 ---\n",
    "st.sidebar.header(\"🔑 API 설정\")\n",
    "provider = st.sidebar.selectbox(\"Provider\", [\"openai\", \"azure\", \"openai-legacy\"], index=0)\n",
    "openai_key = st.sidebar.text_input(\"OPENAI_API_KEY\", type=\"password\", value=os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
    "gmaps_key  = st.sidebar.text_input(\"GMAPS_API_KEY (선택)\", type=\"password\", value=os.getenv(\"GMAPS_API_KEY\", \"\"))\n",
    "tavily_key = st.sidebar.text_input(\"TAVILY_API_KEY (선택)\", type=\"password\", value=os.getenv(\"TAVILY_API_KEY\", \"\"))\n",
    "\n",
    "if openai_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "if gmaps_key:\n",
    "    os.environ[\"GMAPS_API_KEY\"] = gmaps_key\n",
    "if tavily_key:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = tavily_key\n",
    "\n",
    "use_tavily = bool(tavily_key and tavily_key.strip())\n",
    "st.sidebar.success(f\"Provider={provider} | Tavily={'ON' if use_tavily else 'OFF'}\")\n",
    "\n",
    "st.title(\"📸 덕픽 멀티 이미지 분석 (Chat 스타일)\")\n",
    "st.caption(\"여러 장을 한번에 올리고, 각 이미지마다 챗 로그처럼 결과를 받으세요. 인물 실명/신원은 식별하지 않습니다.\")\n",
    "\n",
    "uploaded_files = st.file_uploader(\n",
    "    \"여러 이미지를 선택하세요\",\n",
    "    type=[\"jpg\",\"jpeg\",\"png\",\"webp\",\"bmp\",\"tif\",\"tiff\"],\n",
    "    accept_multiple_files=True\n",
    ")\n",
    "\n",
    "col_left, col_right = st.columns([1, 1])\n",
    "\n",
    "if uploaded_files:\n",
    "    with st.spinner(\"분석 준비 중...\"):\n",
    "        tempdir = Path(tempfile.mkdtemp(prefix=\"dukpick_\"))\n",
    "        paths = []\n",
    "        for f in uploaded_files:\n",
    "            p = tempdir / f.name\n",
    "            with open(p, \"wb\") as out:\n",
    "                out.write(f.read())\n",
    "            paths.append(p)\n",
    "\n",
    "    st.success(f\"{len(paths)}개 이미지 업로드 완료. 분석을 시작하세요.\")\n",
    "\n",
    "    if st.button(\"🚀 모두 분석\"):\n",
    "        for i, p in enumerate(paths, 1):\n",
    "            st.markdown(f\"---\\n#### [{i}/{len(paths)}] {p.name}\")\n",
    "            try:\n",
    "                img = Image.open(p)\n",
    "                col_left, col_right = st.columns([1, 1])\n",
    "                with col_left:\n",
    "                    st.image(img, caption=p.name, use_container_width=True)\n",
    "\n",
    "                with col_right:\n",
    "                    res = process_image(str(p), os.getenv(\"GMAPS_API_KEY\"), os.getenv(\"OPENAI_API_KEY\"))\n",
    "                    chat_text = format_chat_style(res)\n",
    "                    # 챗 버블 느낌\n",
    "                    with st.chat_message(\"assistant\"):\n",
    "                        st.markdown(chat_text)\n",
    "                    with st.expander(\"원본 결과(JSON) 보기\", expanded=False):\n",
    "                        st.json(res)\n",
    "            except Exception as e:\n",
    "                st.error(f\"분석 실패: {e}\")\n",
    "\n",
    "else:\n",
    "    st.info(\"좌측에서 API 키를 확인하고, 위에서 이미지를 여러 장 올려주세요.\")\n",
    "\n",
    "st.caption(\"※ 개인 키는 코드/깃에 절대 커밋하지 마세요. 환경변수(.env) 사용 권장.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920d50e",
   "metadata": {},
   "source": [
    "5단계 결과 PDF 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70413e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📄 data/ 아래 모든 이미지 → (왼쪽) 사진 / (오른쪽) 결과  PDF 생성기\n",
    "# 필요: pip install reportlab pillow\n",
    "import os, json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from reportlab.lib.pagesizes import A4, landscape\n",
    "from reportlab.lib.units import mm\n",
    "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Image as RLImage, Paragraph, Spacer, PageBreak\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "\n",
    "# ✅ 한글 폰트 등록 (없으면 Helvetica로 대체됩니다)\n",
    "FONT_PATH = os.getenv(\"KR_FONT_PATH\", \"\")  # 예) \"/usr/share/fonts/truetype/noto/NotoSansKR-Regular.otf\"\n",
    "BASE_FONT = \"Helvetica\"\n",
    "if FONT_PATH and os.path.exists(FONT_PATH):\n",
    "    pdfmetrics.registerFont(TTFont(\"KRFont\", FONT_PATH))\n",
    "    BASE_FONT = \"KRFont\"\n",
    "\n",
    "styles = getSampleStyleSheet()\n",
    "styles.add(ParagraphStyle(name=\"BodyKR\", parent=styles[\"Normal\"], fontName=BASE_FONT, fontSize=9, leading=12))\n",
    "styles.add(ParagraphStyle(name=\"TitleKR\", parent=styles[\"Heading2\"], fontName=BASE_FONT))\n",
    "\n",
    "def _fit_image_keep_aspect(path, target_w):\n",
    "    with Image.open(path) as im:\n",
    "        w, h = im.size\n",
    "    target_h = h * (target_w / w)\n",
    "    return target_w, target_h\n",
    "\n",
    "def _nz(x, dash=\"—\"):\n",
    "    return x if (x not in (None, \"\", [])) else dash\n",
    "\n",
    "def _trim(text, n=900):\n",
    "    t = (str(text) if text is not None else \"\").strip()\n",
    "    return t if len(t) <= n else t[:n-1] + \"…\"\n",
    "\n",
    "def _right_panel_html(res):\n",
    "    ec = res.get(\"event_card\") or {}\n",
    "    sources = ec.get(\"sources\") or []\n",
    "    src_html = \"<br/>\".join([\n",
    "        f'• <a href=\"{s.get(\"url\")}\">{_trim(s.get(\"title\") or s.get(\"url\"), 80)}</a>'\n",
    "        for s in sources[:6]\n",
    "    ]) or \"—\"\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f'<b>파일:</b> {os.path.basename(res.get(\"image_path\",\"\"))}')\n",
    "    lines.append(f'<b>EXIF 시간:</b> {_nz(res.get(\"exif_datetime\"))}')\n",
    "    lat, lon = res.get(\"lat\"), res.get(\"lon\")\n",
    "    lines.append(f'<b>좌표:</b> {lat if lat is not None else \"—\"}, {lon if lon is not None else \"—\"}')\n",
    "    lines.append(f'<b>장소:</b> {_nz(res.get(\"place_name\"))}')\n",
    "\n",
    "    if res.get(\"place_description\"):\n",
    "        lines.append(f'<br/><b>장소 소개</b><br/>{_trim(res.get(\"place_description\"), 800)}')\n",
    "    if res.get(\"analysis\"):\n",
    "        lines.append(f'<br/><b>이미지 분석 요약</b><br/>{_trim(res.get(\"analysis\"), 1200)}')\n",
    "\n",
    "    if ec:\n",
    "        lines.append(\"<br/><b>EventCard</b>\")\n",
    "        lines.append(f'• <b>Method:</b> {_nz(ec.get(\"method\"))}')\n",
    "        if ec.get(\"event_title\"): lines.append(f'• <b>제목:</b> {ec.get(\"event_title\")}')\n",
    "        if ec.get(\"event_type\"):  lines.append(f'• <b>유형:</b> {ec.get(\"event_type\")}')\n",
    "        if ec.get(\"occurred_time_utc\"):\n",
    "            lines.append(f'• <b>발생시각(UTC):</b> {ec.get(\"occurred_time_utc\")} '\n",
    "                         f'(conf={ec.get(\"occurred_time_confidence\",0):.2f})')\n",
    "        if ec.get(\"location_name\"): lines.append(f'• <b>장소명:</b> {ec.get(\"location_name\")}')\n",
    "        if ec.get(\"what_happened\"): lines.append(f'• <b>무엇:</b> {_trim(ec.get(\"what_happened\"), 600)}')\n",
    "        if ec.get(\"why_summary\"):  lines.append(f'• <b>왜:</b> {_trim(ec.get(\"why_summary\"), 600)}')\n",
    "        if ec.get(\"who_involved\"): lines.append(f'• <b>누가:</b> {_trim(ec.get(\"who_involved\"), 400)}')\n",
    "        if ec.get(\"key_evidence\"):\n",
    "            kev = \"<br/>\".join([f\" - {_trim(k,120)}\" for k in ec.get(\"key_evidence\")[:6]])\n",
    "            lines.append(f'<b>근거:</b><br/>{kev}')\n",
    "        lines.append(f'<b>출처:</b><br/>{src_html}')\n",
    "        if ec.get(\"notes\"): lines.append(f'<br/><b>Notes:</b> {_trim(ec.get(\"notes\"), 400)}')\n",
    "\n",
    "    return \"<br/>\".join(lines)\n",
    "\n",
    "def build_pdf_from_data(data_dir=\"data\", out_pdf=\"outputs/report.pdf\",\n",
    "                        col_ratio=(0.45, 0.55),\n",
    "                        margins=(12*mm, 12*mm, 12*mm, 12*mm)):\n",
    "    \"\"\"\n",
    "    data_dir 아래의 모든 이미지 파일을 순회하며:\n",
    "      - process_image(...) 실행\n",
    "      - 각 파일을 한 페이지에 (좌)이미지, (우)결과 텍스트 형태로 배치\n",
    "      - PDF 및 원시 결과 JSON 저장\n",
    "    ※ API_CFG는 앞선 'API 설정 셀'에서 만든 dict를 사용합니다.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_pdf), exist_ok=True)\n",
    "\n",
    "    # 이미지 수집\n",
    "    exts = (\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\",\".tif\",\".tiff\")\n",
    "    paths = [p for p in sorted(glob(os.path.join(data_dir, \"**\", \"*\"), recursive=True))\n",
    "             if p.lower().endswith(exts)]\n",
    "    if not paths:\n",
    "        raise RuntimeError(f\"이미지 파일이 없습니다: {data_dir}\")\n",
    "\n",
    "    # 전체 처리\n",
    "    results = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            r = process_image(p, API_CFG.get(\"gmaps_api_key\"), API_CFG.get(\"openai_api_key\"))\n",
    "        except Exception as e:\n",
    "            r = {\"image_path\": p, \"error\": str(e), \"event_card\": {\"method\": \"N/A\"}}\n",
    "        results.append(r)\n",
    "\n",
    "    # 원시 결과 저장(참고용)\n",
    "    with open(out_pdf.replace(\".pdf\", \".json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # PDF 조립\n",
    "    doc = SimpleDocTemplate(\n",
    "        out_pdf,\n",
    "        pagesize=landscape(A4),\n",
    "        leftMargin=margins[0], rightMargin=margins[1],\n",
    "        topMargin=margins[2], bottomMargin=margins[3]\n",
    "    )\n",
    "    PAGE_W, PAGE_H = landscape(A4)\n",
    "    content_w = PAGE_W - (margins[0] + margins[1])\n",
    "    left_w  = content_w * col_ratio[0]\n",
    "    right_w = content_w * col_ratio[1]\n",
    "\n",
    "    story = []\n",
    "    for r in results:\n",
    "        img_path = r[\"image_path\"]\n",
    "        # 왼쪽: 이미지\n",
    "        try:\n",
    "            iw, ih = _fit_image_keep_aspect(img_path, left_w)\n",
    "            img_flow = RLImage(img_path, width=iw, height=ih)\n",
    "        except Exception:\n",
    "            img_flow = Paragraph(\"<b>이미지 로드 실패</b>\", styles[\"BodyKR\"])\n",
    "\n",
    "        # 오른쪽: 결과 텍스트\n",
    "        right_html = _right_panel_html(r)\n",
    "        right_para = Paragraph(right_html, styles[\"BodyKR\"])\n",
    "\n",
    "        table = Table([[img_flow, right_para]], colWidths=[left_w, right_w], hAlign=\"LEFT\")\n",
    "        table.setStyle(TableStyle([\n",
    "            (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
    "            (\"LEFTPADDING\", (0,0), (-1,-1), 6),\n",
    "            (\"RIGHTPADDING\", (0,0), (-1,-1), 6),\n",
    "            (\"TOPPADDING\", (0,0), (-1,-1), 6),\n",
    "            (\"BOTTOMPADDING\", (0,0), (-1,-1), 6),\n",
    "        ]))\n",
    "\n",
    "        title = Paragraph(f\"<b>분석 리포트</b> — {os.path.basename(img_path)}\", styles[\"TitleKR\"])\n",
    "        story += [title, Spacer(1, 4*mm), table, PageBreak()]\n",
    "\n",
    "    doc.build(story)\n",
    "    return out_pdf, results\n",
    "\n",
    "#사용 예:\n",
    "pdf_path, results = build_pdf_from_data(\"data\", \"outputs/report.pdf\")\n",
    "print(\"PDF 생성:\", pdf_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
