{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf99150",
   "metadata": {},
   "source": [
    "1 ë‹¨ê³„ ë­ì²´ì¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea07068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain>=0.2\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: pydantic in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (2.11.7)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (10.2.0)\n",
      "Collecting googlemaps\n",
      "  Downloading googlemaps-4.10.0.tar.gz (33 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tavily-python\n",
      "  Downloading tavily_python-0.7.10-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (2.9.0.post0)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain>=0.2)\n",
      "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain>=0.2)\n",
      "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain>=0.2)\n",
      "  Downloading langsmith-0.4.14-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain>=0.2) (2.0.25)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain>=0.2) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain>=0.2) (6.0.1)\n",
      "Collecting openai<2.0.0,>=1.99.9 (from langchain-openai)\n",
      "  Downloading openai-1.99.9-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (3.9.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from langchain-community) (8.5.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from pydantic) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from pydantic) (0.4.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from tavily-python) (0.28.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil) (1.17.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain>=0.2)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain>=0.2) (25.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from langsmith>=0.1.17->langchain>=0.2) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain>=0.2) (1.0.0)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain>=0.2)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from httpx->tavily-python) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->tavily-python) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from httpx->tavily-python) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->tavily-python) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx->tavily-python) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.8.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain>=0.2) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain>=0.2) (2.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2) (3.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2023.10.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain>=0.2) (2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ì •í•˜ë¯¼\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>4->openai<2.0.0,>=1.99.9->langchain-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.4/1.0 MB 12.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.0/1.0 MB 15.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 12.9 MB/s eta 0:00:00\n",
      "Downloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
      "   ---------------------------------------- 0.0/74.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 74.4/74.4 kB ? eta 0:00:00\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.5 MB 15.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.1/2.5 MB 14.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.7/2.5 MB 15.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.3/2.5 MB 16.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 14.7 MB/s eta 0:00:00\n",
      "Downloading tavily_python-0.7.10-py3-none-any.whl (15 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
      "   ---------------------------------------- 0.0/443.5 kB ? eta -:--:--\n",
      "   --------------------------------------  440.3/443.5 kB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 443.5/443.5 kB 13.5 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Downloading langsmith-0.4.14-py3-none-any.whl (373 kB)\n",
      "   ---------------------------------------- 0.0/373.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 373.2/373.2 kB 22.7 MB/s eta 0:00:00\n",
      "Downloading openai-1.99.9-py3-none-any.whl (786 kB)\n",
      "   ---------------------------------------- 0.0/786.8 kB ? eta -:--:--\n",
      "   ------------------------- ------------- 522.2/786.8 kB 16.0 MB/s eta 0:00:01\n",
      "   --------------------------------------- 786.8/786.8 kB 12.3 MB/s eta 0:00:00\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.2/45.2 kB ? eta 0:00:00\n",
      "Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl (884 kB)\n",
      "   ---------------------------------------- 0.0/884.4 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 297.0/884.4 kB 9.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 706.6/884.4 kB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 884.4/884.4 kB 9.3 MB/s eta 0:00:00\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.9/50.9 kB ? eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl (495 kB)\n",
      "   ---------------------------------------- 0.0/495.4 kB ? eta -:--:--\n",
      "   --------------------------------------  491.5/495.4 kB 15.0 MB/s eta 0:00:01\n",
      "   --------------------------------------- 495.4/495.4 kB 10.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: googlemaps\n",
      "  Building wheel for googlemaps (setup.py): started\n",
      "  Building wheel for googlemaps (setup.py): finished with status 'done'\n",
      "  Created wheel for googlemaps: filename=googlemaps-4.10.0-py3-none-any.whl size=40746 sha256=f515da604a454caf11fe0e4e356fa67c7b559942f449ee5852f4863623c749ca\n",
      "  Stored in directory: c:\\users\\ì •í•˜ë¯¼\\appdata\\local\\pip\\cache\\wheels\\f1\\09\\77\\3cc2f5659cbc62341b30f806aca2b25e6a26c351daa5b1f49a\n",
      "Successfully built googlemaps\n",
      "Installing collected packages: zstandard, typing-inspect, marshmallow, jsonpatch, httpx-sse, tiktoken, googlemaps, dataclasses-json, tavily-python, pydantic-settings, openai, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.98.0\n",
      "    Uninstalling openai-1.98.0:\n",
      "      Successfully uninstalled openai-1.98.0\n",
      "Successfully installed dataclasses-json-0.6.7 googlemaps-4.10.0 httpx-sse-0.4.1 jsonpatch-1.33 langchain-0.3.27 langchain-community-0.3.27 langchain-core-0.3.74 langchain-openai-0.3.30 langchain-text-splitters-0.3.9 langsmith-0.4.14 marshmallow-3.26.1 openai-1.99.9 pydantic-settings-2.10.1 tavily-python-0.7.10 tiktoken-0.11.0 typing-inspect-0.9.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script openai.exe is installed in 'C:\\Users\\ì •í•˜ë¯¼\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install \"langchain>=0.2\" langchain-openai langchain-community pydantic pillow googlemaps tavily-python python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b57cf982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "process_image()ì˜ ê¸°ì¡´ íë¦„/ì‹œê·¸ë‹ˆì²˜ë¥¼ ìœ ì§€í•˜ë©´ì„œ,\n",
    "LangChainì„ ì¨ì„œ 'ì¥ì†Œ ì„¤ëª…'ì„ ë„˜ì–´ 'ë¬´ìŠ¨ ì´ë²¤íŠ¸ê°€ ì–¸ì œ/ì™œ ì¼ì–´ë‚¬ëŠ”ì§€'ê¹Œì§€\n",
    "ì¶”ë¡ /ê²€ìƒ‰í•´ EventCardë¡œ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” ë“œë¡­ì¸ êµì²´ ë²„ì „.\n",
    "\n",
    "ê²€ìƒ‰ì€ **Tavilyë§Œ** ì‚¬ìš©í•©ë‹ˆë‹¤. (tavily-python + langchain-community)\n",
    "\n",
    "í•„ìš” íŒ¨í‚¤ì§€\n",
    "pip install \"langchain>=0.2\" langchain-openai langchain-community pydantic pillow googlemaps tavily-python python-dateutil\n",
    "\n",
    "í™˜ê²½ë³€ìˆ˜(ê¶Œì¥)\n",
    "- OPENAI_API_KEY : OpenAI í‚¤ (í•¨ìˆ˜ ì¸ì openai_api_keyë¡œë„ ì£¼ì… ê°€ëŠ¥)\n",
    "- TAVILY_API_KEY : Tavily í‚¤(ì—†ìœ¼ë©´ ê²€ìƒ‰ ìƒëµ)\n",
    "\n",
    "ê¸°ì¡´ ë°˜í™˜ê°’ì„ ë³´ì¡´í•˜ë©´ì„œ, result[\"event_card\"]ë¥¼ ì¶”ê°€ë¡œ ëŒë ¤ì¤ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "from typing import Optional, List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import os\n",
    "import base64\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from PIL import Image, ExifTags\n",
    "from dateutil import parser as dateparser\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# LangChain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "try:\n",
    "    # ê²€ìƒ‰ì€ Tavilyë§Œ!\n",
    "    from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "    _HAVE_TAVILY = True\n",
    "except Exception:  # pragma: no cover\n",
    "    _HAVE_TAVILY = False\n",
    "\n",
    "# =====================\n",
    "# OpenAI/Azure/Legacy API ìŠ¤ìœ„ì²˜ (chat.completions.create í†µì¼ ì¸í„°í˜ì´ìŠ¤)\n",
    "# =====================\n",
    "from typing import Callable\n",
    "import openai as _openai_legacy\n",
    "\n",
    "def get_chat_create(\n",
    "    api: str = \"openai\",                # \"openai\" | \"legacy\" | \"azure\"\n",
    "    api_key: Optional[str] = None,\n",
    "    base_url: Optional[str] = None,     # Azure endpoint ë“±\n",
    "    api_version: Optional[str] = None,  # Azure API ë²„ì „ (ì˜ˆ: \"2024-02-15-preview\")\n",
    ") -> Callable[..., Any]:\n",
    "    \"\"\"\n",
    "    ë°˜í™˜: chat.completions.create(**kwargs)ì²˜ëŸ¼ í˜¸ì¶œ ê°€ëŠ¥í•œ í•¨ìˆ˜.\n",
    "    ê¸°ì¡´ ì½”ë“œì˜ client.chat.completions.create ìë¦¬ì— ê·¸ëŒ€ë¡œ ë„£ì–´ ì“¸ ìˆ˜ ìˆìŒ.\n",
    "    \"\"\"\n",
    "    if api == \"openai\":\n",
    "        try:\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI(api_key=api_key, base_url=base_url) if base_url else OpenAI(api_key=api_key)\n",
    "            return client.chat.completions.create\n",
    "        except Exception:\n",
    "            _openai_legacy.api_key = api_key\n",
    "            def _create(**kwargs):\n",
    "                return _openai_legacy.ChatCompletion.create(**kwargs)\n",
    "            return _create\n",
    "    elif api == \"legacy\":\n",
    "        _openai_legacy.api_key = api_key\n",
    "        def _create(**kwargs):\n",
    "            return _openai_legacy.ChatCompletion.create(**kwargs)\n",
    "        return _create\n",
    "    elif api == \"azure\":\n",
    "        from openai import AzureOpenAI\n",
    "        client = AzureOpenAI(api_key=api_key, azure_endpoint=base_url, api_version=api_version or \"2024-02-15-preview\")\n",
    "        return client.chat.completions.create\n",
    "    else:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=api_key, base_url=base_url) if base_url else OpenAI(api_key=api_key)\n",
    "        return client.chat.completions.create\n",
    "\n",
    "# =====================\n",
    "# LangChain ê¸°ë°˜ ì´ë²¤íŠ¸ í™•ì¥ ìœ í‹¸ (ê²€ìƒ‰ì€ Tavilyë§Œ)\n",
    "# =====================\n",
    "\n",
    "class VisionClues(BaseModel):\n",
    "    scene_summary: str\n",
    "    visible_text: List[str] = []\n",
    "    notable_objects: List[str] = []\n",
    "    time_clues: List[str] = []\n",
    "    place_clues: List[str] = []\n",
    "    event_guess: Optional[str] = None\n",
    "    candidate_queries: List[str] = []\n",
    "\n",
    "\n",
    "class SourceItem(BaseModel):\n",
    "    url: str\n",
    "    title: Optional[str] = None\n",
    "\n",
    "\n",
    "class EventCard(BaseModel):\n",
    "    method: str\n",
    "    event_title: Optional[str] = None\n",
    "    event_type: Optional[str] = None\n",
    "    occurred_time_utc: Optional[str] = None\n",
    "    occurred_time_confidence: float = 0.0\n",
    "    location_name: Optional[str] = None\n",
    "    latitude: Optional[float] = None\n",
    "    longitude: Optional[float] = None\n",
    "    why_summary: Optional[str] = None\n",
    "    what_happened: Optional[str] = None\n",
    "    who_involved: Optional[str] = None\n",
    "    key_evidence: List[str] = []\n",
    "    sources: List[SourceItem] = []\n",
    "    notes: Optional[str] = None\n",
    "\n",
    "\n",
    "def _set_openai_key(openai_api_key: Optional[str]):\n",
    "    if openai_api_key:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "\n",
    "def _b64image(image_path: str) -> str:\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19fbd7",
   "metadata": {},
   "source": [
    "2ë‹¨ê³„ í•„ìˆ˜ í•¨ìˆ˜ë“¤ ìˆ˜ë¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bd28f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# ê¸°ì¡´ í—¬í¼ (ì‹œê·¸ë‹ˆì²˜ ìœ ì§€)\n",
    "# =====================\n",
    "\n",
    "def has_gps_metadata(image_path: str) -> bool:\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        exif_raw = img._getexif() or {}\n",
    "        if not exif_raw:\n",
    "            return False\n",
    "        tagmap = {ExifTags.TAGS.get(k, k): v for k, v in (exif_raw or {}).items()}\n",
    "        return \"GPSInfo\" in tagmap\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _dms_to_deg(dms, ref):\n",
    "    def _r(x):\n",
    "        return float(x[0]) / float(x[1]) if isinstance(x, tuple) else float(x)\n",
    "    deg = _r(dms[0]) + _r(dms[1]) / 60.0 + _r(dms[2]) / 3600.0\n",
    "    if ref in [\"S\", \"W\"]:\n",
    "        deg = -deg\n",
    "    return deg\n",
    "\n",
    "\n",
    "def extract_gps(image_path: str):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        exif_raw = img._getexif() or {}\n",
    "        tagmap = {ExifTags.TAGS.get(k, k): v for k, v in (exif_raw or {}).items()}\n",
    "        gps = tagmap.get(\"GPSInfo\")\n",
    "        if not gps:\n",
    "            return None, None\n",
    "        gps = {ExifTags.GPSTAGS.get(k, k): v for k, v in gps.items()}\n",
    "        lat_dms, lat_ref = gps.get(\"GPSLatitude\"), gps.get(\"GPSLatitudeRef\")\n",
    "        lon_dms, lon_ref = gps.get(\"GPSLongitude\"), gps.get(\"GPSLongitudeRef\")\n",
    "        if lat_dms and lat_ref and lon_dms and lon_ref:\n",
    "            return _dms_to_deg(lat_dms, lat_ref), _dms_to_deg(lon_dms, lon_ref)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def _extract_exif_datetime(image_path: str) -> Optional[datetime]:\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        exif_raw = img._getexif() or {}\n",
    "        tagmap = {ExifTags.TAGS.get(k, k): v for k, v in (exif_raw or {}).items()}\n",
    "        for key in (\"DateTimeOriginal\", \"DateTimeDigitized\", \"DateTime\"):\n",
    "            if key in tagmap:\n",
    "                try:\n",
    "                    return dateparser.parse(str(tagmap[key]).replace(\":\", \"-\", 2))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def reverse_geocode(lat: float, lon: float, gmaps_api_key: str) -> Optional[str]:\n",
    "    try:\n",
    "        import googlemaps\n",
    "        gmaps = googlemaps.Client(key=gmaps_api_key)\n",
    "        res = gmaps.reverse_geocode((lat, lon), language=\"ko\")\n",
    "        if res:\n",
    "            return res[0].get(\"formatted_address\")\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def _extract_vision_clues(image_path: str, openai_api_key: Optional[str]) -> VisionClues:\n",
    "    _set_openai_key(openai_api_key)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    # ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ë¥¼ ì§ì ‘ êµ¬ì„± (PromptTemplate ëŒ€ì‹ )\n",
    "    system = SystemMessage(content=(\n",
    "        \"ë„ˆëŠ” ì´ë¯¸ì§€ ë¶„ì„ê°€ë‹¤. ì´ë²¤íŠ¸ ì‹ë³„ì— í•„ìš”í•œ ë‹¨ì„œë§Œ ê°„ê²°íˆ ì¶”ì¶œí•˜ë¼.\\n\"\n",
    "        \"- ê°„íŒ/í˜„ìˆ˜ë§‰ í…ìŠ¤íŠ¸, ë¡œê³ /ìœ ë‹ˆí¼/ê¹ƒë°œ ë“± íŠ¹ì§• ìš”ì†Œ\\n\"\n",
    "        \"- ì‹œê°„ ë‹¨ì„œ(ë‚®/ë°¤/ê³„ì ˆ/ì—°ë„í‘œì‹œ/ì¥ì‹)\\n\"\n",
    "        \"- ì¥ì†Œ ë‹¨ì„œ(ì–¸ì–´/ëœë“œë§ˆí¬/ë„ë¡œí‘œì§€)\\n\"\n",
    "        \"- 3~6ê°œì˜ ê²€ìƒ‰ ì§ˆì˜ í›„ë³´ë¥¼ í•œêµ­ì–´/ì˜ì–´ë¡œ ì œì•ˆ\\n\"\n",
    "        \"ì¶œë ¥ì€ JSON(ëª¨ë¸ í•¨ìˆ˜í˜¸ì¶œ)ìœ¼ë¡œ.\"\n",
    "    ))\n",
    "    human = HumanMessage(content=[\n",
    "        {\"type\": \"text\", \"text\": \"ì´ ì´ë¯¸ì§€ì—ì„œ ì´ë²¤íŠ¸ ì‹ë³„ ë‹¨ì„œë¥¼ ì¶”ì¶œí•´ì¤˜.\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,\" + _b64image(image_path)}},\n",
    "    ])\n",
    "    structured = llm.with_structured_output(VisionClues)\n",
    "    return structured.invoke([system, human])\n",
    "\n",
    "\n",
    "def _run_search(queries: List[str], k: int = 6) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Tavilyë§Œ ì‚¬ìš©. í‚¤ê°€ ì—†ê±°ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰ ìƒëµ.\n",
    "    \"\"\"\n",
    "    docs: List[Document] = []\n",
    "    if not _HAVE_TAVILY or not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "        return docs\n",
    "    tool = TavilySearchResults(k=min(k, 8), include_images=False)\n",
    "    for q in queries[:6]:\n",
    "        q = (q or \"\").strip()\n",
    "        if not q:\n",
    "            continue\n",
    "        try:\n",
    "            for r in tool.invoke({\"query\": q}):\n",
    "                docs.append(Document(\n",
    "                    page_content=r.get(\"content\", \"\"),\n",
    "                    metadata={\"source\": r.get(\"url\"), \"title\": r.get(\"title\")}\n",
    "                ))\n",
    "        except Exception:\n",
    "            pass\n",
    "    # ì¤‘ë³µ URL ì œê±°\n",
    "    uniq, seen = [], set()\n",
    "    for d in docs:\n",
    "        u = d.metadata.get(\"source\")\n",
    "        if not u or u in seen:\n",
    "            continue\n",
    "        seen.add(u)\n",
    "        uniq.append(d)\n",
    "    return uniq[:k]\n",
    "\n",
    "\n",
    "def _synthesize_event_card(\n",
    "    clues: VisionClues,\n",
    "    search_docs: List[Document],\n",
    "    lat: Optional[float],\n",
    "    lon: Optional[float],\n",
    "    place_name: Optional[str],\n",
    "    exif_dt: Optional[datetime],\n",
    "    openai_api_key: Optional[str],\n",
    ") -> EventCard:\n",
    "    _set_openai_key(openai_api_key)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    docs_text = []\n",
    "    for d in search_docs[:8]:\n",
    "        docs_text.append(\n",
    "            f\"- {d.metadata.get('title') or ''} | {d.metadata.get('source') or ''}\\n  {d.page_content[:500]}\"\n",
    "        )\n",
    "    docs_blob = \"\\n\".join(docs_text)\n",
    "\n",
    "    system = SystemMessage(content=(\n",
    "        \"ë„ˆëŠ” ë””ì§€í„¸ í¬ë Œì‹ ë¶„ì„ê°€ë‹¤. ë‹¨ì„œì™€ ê²€ìƒ‰ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ë²¤íŠ¸ ì¹´ë“œë¥¼ ì‘ì„±í•˜ë¼.\\n\"\n",
    "        \"- ì‹œê°„ì€ UTC ISO8601 í•˜ë‚˜ë¡œ. ë¶ˆí™•ì‹¤í•˜ë©´ ê·¼ì‚¬ì™€ ì‹ ë¢°ë„ í‘œê¸°.\\n\"\n",
    "        \"- ë¬´ì—‡/ëˆ„ê°€/ì™œë¥¼ êµ¬ì²´ì ìœ¼ë¡œ.\\n\"\n",
    "        \"- í•µì‹¬ ê·¼ê±°ì™€ ì¶œì²˜ URL í¬í•¨.\\n\"\n",
    "        \"- ê³¼ë„í•œ í™•ì‹ ì€ í”¼í•˜ê³  ë¶ˆí™•ì‹¤ì„±ì€ notesì—.\"\n",
    "    ))\n",
    "    human = HumanMessage(content=(\n",
    "        \"[CLUES]\\n\"\n",
    "        f\"{clues.model_dump()}\\n\\n\"\n",
    "        \"[EXIF]\\n\"\n",
    "        f\"lat={lat} lon={lon} place={place_name} exif_time={exif_dt.isoformat() if exif_dt else None}\\n\\n\"\n",
    "        \"[DOCS]\\n\"\n",
    "        f\"{docs_blob}\\n\\n\"\n",
    "        \"ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•´ EventCard(JSON)ë§Œ ë°˜í™˜.\"\n",
    "    ))\n",
    "    structured = llm.with_structured_output(EventCard)\n",
    "    card: EventCard = structured.invoke([system, human])\n",
    "\n",
    "    methods = []\n",
    "    if lat is not None and lon is not None:\n",
    "        methods.append(\"EXIF\")\n",
    "    if clues:\n",
    "        methods.append(\"VISION\")\n",
    "    if search_docs:\n",
    "        methods.append(\"SEARCH\")\n",
    "    card.method = \"+\".join(methods) or \"VISION\"\n",
    "\n",
    "    # ì†ŒìŠ¤ê°€ ë¹„ì–´ìˆë‹¤ë©´ ê²€ìƒ‰ URL ì±„ìš°ê¸°\n",
    "    if (not card.sources) and search_docs:\n",
    "        for d in search_docs[:5]:\n",
    "            u = d.metadata.get(\"source\")\n",
    "            if u:\n",
    "                card.sources.append(SourceItem(url=u, title=d.metadata.get(\"title\")))\n",
    "\n",
    "    return card\n",
    "\n",
    "# =====================\n",
    "# ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€: GPT ì„¤ëª…ê¸°\n",
    "# =====================\n",
    "\n",
    "def explain_place_with_gpt(place_name: str, lat: float, lon: float, openai_api_key: str) -> str:\n",
    "    \"\"\"ì¥ì†Œ ì„¤ëª… ì¤‘ì‹¬ì˜ ì§§ì€ ë‚´ëŸ¬í‹°ë¸Œ(ê¸°ì¡´ í˜¸í™˜).\"\"\"\n",
    "    _set_openai_key(openai_api_key)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "    system = SystemMessage(content=\"í˜„ì§€ ê°€ì´ë“œì²˜ëŸ¼ ê°„ê²°í•˜ê²Œ ì¥ì†Œë¥¼ ì†Œê°œí•´ë¼. í˜„ì¬ì„±ì— ë¯¼ê°í•œ ì •ë³´ëŠ” ì¼ë°˜ì  ì‚¬ì‹¤ ìœ„ì£¼ë¡œ.\")\n",
    "    human = HumanMessage(content=(\n",
    "        \"ë‹¤ìŒ ì¢Œí‘œì˜ ì¥ì†Œì— ëŒ€í•´ 4~6ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ ì†Œê°œí•´ì¤˜.\\n\"\n",
    "        f\"ì¥ì†Œëª…: {place_name}\\n\"\n",
    "        f\"ì¢Œí‘œ: {lat}, {lon}\"\n",
    "    ))\n",
    "    return llm.invoke([system, human]).content\n",
    "\n",
    "\n",
    "def analyze_image_with_gpt(image_path: str, openai_api_key: str, *,\n",
    "                           model: str = \"gpt-4o\",\n",
    "                           api: str = \"openai\",                # \"openai\" | \"legacy\" | \"azure\"\n",
    "                           base_url: Optional[str] = None,\n",
    "                           api_version: Optional[str] = None,\n",
    "                           person_hint: Optional[str] = None,\n",
    "                           location_candidates: int = 3) -> str:\n",
    "    \"\"\"ë©”íƒ€ë°ì´í„° ì—†ì„ ë•Œ ì¥ë©´+ì¥ì†Œ/ì‹œê°„/ì´ë²¤íŠ¸/ëª©ì ì„ ìì—°ì–´ë¡œ ìš”ì•½(ìš”ì²­í•œ ì¶œë ¥ ìŠ¤íƒ€ì¼)í•©ë‹ˆë‹¤.\n",
    "    - ë°˜í™˜ì€ ë¬¸ìì—´ì´ë©°, ì˜ˆì‹œì²˜ëŸ¼ 1~5ë²ˆ í•­ëª©ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.\n",
    "    - ì¸ë¬¼ ì‹ ì› íŠ¹ì • êµ¬ì²´í™”(ê·¸ë£¹, ë©¤ë²„ ì´ë¦„ êµ¬ì²´ì  ì–¸ê¸‰).\n",
    "    - API ìŠ¤ìœ„ì²˜(get_chat_create)ë¡œ OpenAI/Legacy/Azureë¥¼ ì†ì‰½ê²Œ ì „í™˜ ê°€ëŠ¥.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        image_data = img_file.read()\n",
    "    image_b64 = base64.b64encode(image_data).decode()\n",
    "\n",
    "    chat_create = get_chat_create(api=api, api_key=openai_api_key, base_url=base_url, api_version=api_version)\n",
    "\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"ë‹¹ì‹ ì€ ì—°ì˜ˆ/ì•„ì´ëŒ ì‚¬ì§„ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ì§„ ì† ì¸ë¬¼ì˜ ì‹ ì›ì„ ìƒˆë¡œ íŠ¹ì •í•˜ê±°ë‚˜ ì´ë¦„ì„ ì¶”ì •í•˜ì„¸ìš”. \"\n",
    "            \"ë°°ê²½ í…ìŠ¤íŠ¸, ê°„íŒ, ìƒ‰ì±„, ê±´ë¬¼ í˜•íƒœ, ë¡œê³ /í˜„ìˆ˜ë§‰, ì˜ìƒ/ì†Œí’ˆ ë“± ë¹„ì‹ë³„ ë‹¨ì„œë¥¼ ê·¼ê±°ë¡œ ì¥ì†Œ/ì‹œê°„/ì´ë²¤íŠ¸/ëª©ì ì„ ì¶”ì •í•˜ì„¸ìš”. \"\n",
    "            \"í™•ì‹ ì´ ë‚®ë‹¤ë©´ í‘œí˜„ì— ì‹ ì¤‘í•¨ì„ ìœ ì§€í•˜ì„¸ìš”.  \"\n",
    "            \"ì´ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì•„ë˜ í•­ëª©ë“¤ì„ ëª¨ë‘ ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì—ì„œ ì¶”ì •í•´ì¤˜.\\n\\n\"\n",
    "            \"1ë²ˆì€ ë¬´ì¡°ê±´ ì ìœ¼ë¡œ ì–´ë–¤ ì•„ì´ëŒì¸ì§€ ì•Œë ¤ì¤˜, ì†Œì† ê·¸ë£¹ê³¼ ê°™ì´\"\n",
    "                \"1. ì‚¬ì§„ ì† ì¸ë¬¼ì€ ëˆ„êµ¬ì¸ì§€ (ì•„ì´ëŒ ì´ë¦„ê³¼ ì†Œì† ê·¸ë£¹)\\n\"\n",
    "                \"2. ì–´ë””ì—ì„œ ì´¬ì˜ë˜ì—ˆëŠ”ì§€, ì¥ì†Œë¥¼ ìµœëŒ€í•œ ì •í™•í•˜ê²Œ í•´ì¤˜ (ì¥ì†Œëª… + ìœ„ë„/ê²½ë„)\\n\"\n",
    "                \"3. ì–¸ì œì¯¤ ì´¬ì˜ëœ ì‚¬ì§„ì¸ì§€ (ì—°ë„, ê³„ì ˆ ë“±)\\n\"\n",
    "                \"4. ì–´ë–¤ í”„ë¡œê·¸ë¨/ì´ë²¤íŠ¸ì—ì„œ ì°íŒ ê²ƒì¸ì§€\\n\"\n",
    "                \"5. ì™œ ì´¬ì˜ëœ ì‚¬ì§„ì¸ì§€ (ì˜ˆ: ë°©ì†¡, íŒ¬ë¯¸íŒ…, ê´‘ê³ , í™”ë³´ ë“±)\\n\\n\"\n",
    "                \"ì •í™•í•˜ì§€ ì•Šë”ë¼ë„ ì¶”ë¡  ê·¼ê±°ë¥¼ í¬í•¨í•´ì„œ í•œêµ­ì–´ë¡œ ìµœëŒ€í•œ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì¤˜.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    user_text = (\n",
    "        \"ì´ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì•„ë˜ í•­ëª©ë“¤ì„ ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì—ì„œ ì¶”ì •í•´ì¤˜.\\n\\n\"\n",
    "        \"1. ì‚¬ì§„ ì† ì¸ë¬¼: ì‹ ì› íŠ¹ì •ì€ ë¬´ì¡°ê±´ í•´. (ì˜ˆ: 'ì—ìŠ¤íŒŒ ì¹´ë¦¬ë‚˜ë¡œ ë³´ì„'ì²˜ëŸ¼ ì£¼ì²´ì  ì‹ë³„ ì„œìˆ )\\n\"\n",
    "        f\"2. ì´¬ì˜ ì¥ì†Œ: í›„ë³´ {location_candidates}ê°œ ì´ë‚´. ê° í›„ë³´ëŠ” ì¥ì†Œëª…ê³¼ ìœ„ë„/ê²½ë„ë¥¼ í¬í•¨í•˜ê³ , ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ í›„ë³´ë¥¼ ë¨¼ì € ì œì‹œ.\\n\"\n",
    "        \"3. ì´¬ì˜ ì‹œê¸°: ì—°/ì›”(ê°€ëŠ¥í•˜ë©´ ì¼/ì‹œê°„) ë° ê³„ì ˆ, ê·¼ê±° í¬í•¨.\\n\"\n",
    "        \"4. í”„ë¡œê·¸ë¨/ì´ë²¤íŠ¸: ìœ í˜•(ì—¬í–‰/íŒ¬ ì´ë²¤íŠ¸/ë°©ì†¡/ê³µì—° ë“±)ê³¼ ê°€ëŠ¥í•˜ë©´ ì´ë¦„, ê·¼ê±° í¬í•¨.\\n\"\n",
    "        \"5. ì´¬ì˜ ëª©ì : ë°©ì†¡/íŒ¬ì†Œí†µ/ê´‘ê³ /í™”ë³´/í™ë³´ ë“±, ê·¼ê±° í¬í•¨.\\n\\n\"\n",
    "        \"- ë¨¼ì € ê°„ë‹¨í•œ ë¬¸êµ¬ë¡œ ì‹ ì›ì„ ëª…ì‹œ.\\n\"\n",
    "        \"- ì´ì–´ì„œ ë²ˆí˜¸ 1~5ë¡œ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ bullet í˜•ì‹ìœ¼ë¡œ ìš”ì•½.\\n\"\n",
    "        \"- ì¥ì†Œ í•­ëª©ì—ëŠ” ìœ„ë„(latitude), ê²½ë„(longitude) ìˆ«ìë¥¼ í¬í•¨.\\n\"\n",
    "        f\"- ì‚¬ìš©ì ì œê³µ ì¸ë¬¼ íŒíŠ¸(ìˆìœ¼ë©´ ë§¥ë½ë§Œ): {person_hint or 'ì—†ìŒ'}\\n\"\n",
    "    )\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": user_text},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,\" + image_b64}}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = chat_create(\n",
    "        model=model,\n",
    "        messages=[system_msg, user_msg],\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9718cec",
   "metadata": {},
   "source": [
    "3ë‹¨ê³„ ë©”ì¸ ì‘ë™ ë¶€ë¶„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c2424e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# ë©”ì¸: ê¸°ì¡´ process_image() ë“œë¡­ì¸ êµì²´\n",
    "# =====================\n",
    "\n",
    "def _infer_event_card(image_path: str,\n",
    "                      lat: Optional[float], lon: Optional[float],\n",
    "                      place_name: Optional[str],\n",
    "                      exif_dt: Optional[datetime],\n",
    "                      openai_api_key: Optional[str]) -> Dict[str, Any]:\n",
    "    \"\"\"LangChain íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ EventCard ìƒì„± (dict). ê²€ìƒ‰ í‚¤ ì—†ìœ¼ë©´ SEARCH ìƒëµ.\"\"\"\n",
    "    try:\n",
    "        clues = _extract_vision_clues(image_path, openai_api_key)\n",
    "        # ì§ˆì˜ êµ¬ì„± (ì¥ì†Œ/ì¢Œí‘œ/ì—°ë„/ëª¨ë¸ ì¶”ì • ì´ë²¤íŠ¸ í‚¤ì›Œë“œ)\n",
    "        seed = list(clues.candidate_queries or [])\n",
    "        if place_name:\n",
    "            seed.append(f\"{place_name} event news\")\n",
    "        if lat is not None and lon is not None:\n",
    "            seed.append(f\"{lat:.5f},{lon:.5f} event\")\n",
    "        if exif_dt:\n",
    "            seed.append(f\"{exif_dt.year} {place_name or ''} {clues.event_guess or 'event'}\")\n",
    "\n",
    "        # ì¤‘ë³µ ì œê±° & ê¸¸ì´ ì œí•œ\n",
    "        qset, seen = [], set()\n",
    "        for q in seed:\n",
    "            q = (q or \"\").strip()\n",
    "            if not q:\n",
    "                continue\n",
    "            low = q.lower()\n",
    "            if low in seen:\n",
    "                continue\n",
    "            seen.add(low)\n",
    "            qset.append(q[:200])\n",
    "\n",
    "        docs = _run_search(qset, k=6) if qset else []\n",
    "        card = _synthesize_event_card(clues, docs, lat, lon, place_name, exif_dt, openai_api_key)\n",
    "        return card.model_dump()\n",
    "    except Exception as e:\n",
    "        return {\"method\": \"VISION\", \"error\": f\"event inference failed: {e}\"}\n",
    "\n",
    "\n",
    "from datetime import date  # â¬… anchor íƒ€ì…ì— í•„ìš”(ì—†ìœ¼ë©´ ë¹¼ë„ ë¬´ë°©)\n",
    "\n",
    "def process_image(image_path, gmaps_api_key, openai_api_key):\n",
    "    \"\"\"\n",
    "    ê¸°ì¡´ íë¦„/ì‹œê·¸ë‹ˆì²˜ ìœ ì§€:\n",
    "    - EXIF â†’ ì¢Œí‘œ/ì‹œê°„\n",
    "    - Reverse Geocodeë¡œ ì¥ì†Œëª…(ê°€ëŠ¥ì‹œ)\n",
    "    - ì¥ì†Œ ì†Œê°œ(ê°„ë‹¨)\n",
    "    - ì´ë¯¸ì§€ ìš”ì•½(ë©”íƒ€ë°ì´í„° ì—†ì„ ë•Œë„ ë™ì‘)\n",
    "    - EventCard ì¶”ë¡ (+ ì›¹ê²€ìƒ‰ ë³´ê°•)\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“¸ ë¶„ì„ ì¤‘ì¸ ì‚¬ì§„: {image_path}\\n\")\n",
    "\n",
    "    result: Dict[str, Any] = {\"image_path\": image_path}\n",
    "\n",
    "    # EXIF ë‹¨ì„œ\n",
    "    exif_dt = _extract_exif_datetime(image_path)\n",
    "    result[\"exif_datetime\"] = exif_dt.isoformat() if exif_dt else None\n",
    "\n",
    "    lat = lon = None\n",
    "    if has_gps_metadata(image_path):\n",
    "        lat, lon = extract_gps(image_path)\n",
    "    result[\"lat\"] = lat\n",
    "    result[\"lon\"] = lon\n",
    "\n",
    "    # ì—­ì§€ì˜¤ì½”ë”© â†’ ì¥ì†Œëª…\n",
    "    place_name = None\n",
    "    if lat is not None and lon is not None and gmaps_api_key:\n",
    "        place_name = reverse_geocode(lat, lon, gmaps_api_key)\n",
    "    result[\"place_name\"] = place_name\n",
    "\n",
    "    if lat is not None and lon is not None:\n",
    "        print(f\"ğŸ“ ì¢Œí‘œ: {lat:.6f}, {lon:.6f}\")\n",
    "    else:\n",
    "        print(\"ğŸ“ ì¢Œí‘œ: ì—†ìŒ(Exifì— GPS ë¯¸í¬í•¨)\")\n",
    "\n",
    "    if place_name:\n",
    "        print(f\"ğŸ—ºï¸ ì¥ì†Œ: {place_name}\")\n",
    "\n",
    "    # ì¥ì†Œ ê°„ë‹¨ ì†Œê°œ (ê°€ëŠ¥ì‹œ)\n",
    "    place_desc = None\n",
    "    if place_name and (lat is not None) and (lon is not None):\n",
    "        try:\n",
    "            place_desc = explain_place_with_gpt(place_name, lat, lon, openai_api_key)\n",
    "        except Exception as e:\n",
    "            place_desc = f\"(ì¥ì†Œ ì†Œê°œ ì‹¤íŒ¨: {e})\"\n",
    "    result[\"place_description\"] = place_desc\n",
    "\n",
    "    # ì´ë¯¸ì§€ ë¶„ì„ ìš”ì•½ (í•­ìƒ ìˆ˜í–‰)\n",
    "    try:\n",
    "        gpt_analysis = analyze_image_with_gpt(image_path, openai_api_key)\n",
    "    except Exception as e:\n",
    "        gpt_analysis = f\"(ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e})\"\n",
    "    result[\"analysis\"] = gpt_analysis\n",
    "\n",
    "    # 1) EventCard (ê¸°ì¡´ ë°©ì‹; ë‚´ë¶€ì—ì„œ Tavilyë¥¼ ì“¸ ìˆ˜ ìˆìŒ)\n",
    "    result[\"event_card\"] = _infer_event_card(\n",
    "        image_path=image_path,\n",
    "        lat=lat, lon=lon,\n",
    "        place_name=place_name,\n",
    "        exif_dt=exif_dt,\n",
    "        openai_api_key=openai_api_key\n",
    "    )\n",
    "\n",
    "    # 2) === OpenAI ì›¹ê²€ìƒ‰ ë³´ê°• (Responses API: web_search_preview) ===\n",
    "    #    _infer_event_card ê²°ê³¼ì™€ ë³‘í•©í•˜ì—¬ \"VISION+SEARCH\"ë¡œ ìŠ¹ê²©í•˜ê³ , sources ì¶”ê°€\n",
    "    try:\n",
    "        seeds = _build_search_seed(result)              # <- ì‚¬ì „ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨\n",
    "        should_search = bool(seeds[\"has_any\"])\n",
    "        docs = []\n",
    "\n",
    "        # ë‚ ì§œ ê¸°ì¤€ ì¡ê¸°: EXIF ë˜ëŠ” event_cardì˜ occurred_time_utc\n",
    "        anchor: Optional[date] = None\n",
    "        if result.get(\"exif_datetime\"):\n",
    "            d = _parse_iso_utc_loose(result[\"exif_datetime\"])  # <- ì‚¬ì „ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨\n",
    "            if d:\n",
    "                anchor = d.date()\n",
    "\n",
    "        if not anchor:\n",
    "            ec0 = result.get(\"event_card\") or {}\n",
    "            occ = ec0.get(\"occurred_time_utc\")\n",
    "            if isinstance(occ, list) and occ:\n",
    "                occ = occ[0]\n",
    "            d2 = _parse_iso_utc_loose(occ) if isinstance(occ, str) else None\n",
    "            if d2:\n",
    "                anchor = d2.date()\n",
    "\n",
    "        # ê²€ìƒ‰ ì§ˆì˜ ë§Œë“¤ê¸°\n",
    "        if should_search:\n",
    "            top = seeds[\"seeds\"][:3]  # í•µì‹¬ ë‹¨ì„œë§Œ\n",
    "            search_q = \" \".join(top).strip()\n",
    "\n",
    "            if search_q:\n",
    "                print(f\"ğŸ” ì›¹ê²€ìƒ‰ ì§ˆì˜: {search_q}  (anchor={anchor})\")\n",
    "                docs = web_search_documents(                 # <- ì‚¬ì „ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨\n",
    "                    search_q,\n",
    "                    k=5,\n",
    "                    lang=\"ko\",\n",
    "                    anchor_date=anchor,\n",
    "                    window_days=365//2  # anchor Â± 6ê°œì›”\n",
    "                )\n",
    "\n",
    "        # event_card ë³‘í•©\n",
    "        if docs:\n",
    "            ec = result.get(\"event_card\") or {}\n",
    "            ec[\"method\"] = (ec.get(\"method\") or \"VISION\") + \"+SEARCH\"\n",
    "            ec[\"sources\"] = [\n",
    "                {\n",
    "                    \"title\": d[\"title\"],\n",
    "                    \"url\": d[\"url\"],\n",
    "                    \"provider\": \"openai-web\",\n",
    "                    \"published_at\": d.get(\"published_at\")\n",
    "                }\n",
    "                for d in docs if d.get(\"url\")\n",
    "            ]\n",
    "            if not ec.get(\"why_summary\"):\n",
    "                ec[\"why_summary\"] = \"ì›¹ ê²€ìƒ‰ì—ì„œ ì‹œì (anchor) ì£¼ë³€ì˜ ì¶œì²˜ë§Œ ì„ ë³„í•´ ì¥ì†Œ/ì´ë²¤íŠ¸ ë‹¨ì„œë¥¼ ë³´ê°•í–ˆìŠµë‹ˆë‹¤.\"\n",
    "            result[\"event_card\"] = ec\n",
    "\n",
    "            print(\"ğŸ”— ì°¸ê³  ì¶œì²˜(ìƒìœ„):\")\n",
    "            for s in ec[\"sources\"][:3]:\n",
    "                dt = f\" ({s.get('published_at')})\" if s.get(\"published_at\") else \"\"\n",
    "                print(f\"   - {s['title']} | {s['url']}{dt}\")\n",
    "    except Exception as e:\n",
    "        print(f\"(ì›¹ê²€ìƒ‰ ë³´ê°• ìƒëµ/ì‹¤íŒ¨: {e})\")\n",
    "    # ===================================================================\n",
    "\n",
    "    print(\"\\n=== ë¶„ì„ ì™„ë£Œ ===\\n\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519c3ff",
   "metadata": {},
   "source": [
    "4ë‹¨ê³„ ê²°ê³¼ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363786be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í‚¤ ì„¤ì • ì™„ë£Œ | Tavily=ON\n",
      "\n",
      "ğŸ“¸ ë¶„ì„ ì¤‘ì¸ ì‚¬ì§„: C:\\Users\\\\ì •í•˜ë¯¼\\\\Desktop\\\\ë•í”½ í…ŒìŠ¤íŠ¸\\\\data\\\\seventeen_eiffel.jpg\n",
      "\n",
      "ğŸ“ ì¢Œí‘œ: ì—†ìŒ(Exifì— GPS ë¯¸í¬í•¨)\n",
      "ğŸ” ì›¹ê²€ìƒ‰ ì§ˆì˜: UNESCO Headquarters, Paris, France Opening Ceremony of the International Year of Quantum Science and Technology (IYQ) 2025 ì£„ì†¡í•˜ì§€ë§Œ ì´ë¯¸ì§€  (anchor=2025-02-04)\n",
      "ğŸ”— ì°¸ê³  ì¶œì²˜(ìƒìœ„):\n",
      "\n",
      "=== ë¶„ì„ ì™„ë£Œ ===\n",
      "\n",
      "ğŸ“¸ ë¶„ì„ ì¤‘ì¸ ì‚¬ì§„: C:\\Users\\\\ì •í•˜ë¯¼\\\\Desktop\\\\ë•í”½ í…ŒìŠ¤íŠ¸\\\\data\\\\seventeen_eiffel.jpg\n",
      "\n",
      "ğŸ“ ì¢Œí‘œ: ì—†ìŒ(Exifì— GPS ë¯¸í¬í•¨)\n",
      "\n",
      "ğŸ” EXIF ë©”íƒ€ë°ì´í„°ê°€ ê±°ì˜ ì—†ì–´ GPT-4o ì´ë¯¸ì§€ ë¶„ì„ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‚´íˆì–´ìš”. ë˜í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë³´ê°•í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“– ë¶„ì„ ìš”ì•½\n",
      "   - í•œì¤„ ìš”ì•½: ì£„ì†¡í•˜ì§€ë§Œ ì´ë¯¸ì§€ ì† ì¸ë¬¼ì˜ ì‹ ì›ì„ íŠ¹ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. ì‚¬ì§„ ì† ì¸ë¬¼: ì‹ ì› íŠ¹ì • ë¶ˆê°€.\n",
      "2. ì´¬ì˜ ì¥ì†Œ: íŒŒë¦¬, í”„ë‘ìŠ¤ ìœ ë„¤ìŠ¤ì½” ë³¸ë¶€ ê·¼ì²˜ë¡œ ë³´ì…ë‹ˆë‹¤.\n",
      "   - í›„ë³´ 1: ìœ ë„¤ìŠ¤ì½” ë³¸ë¶€, íŒŒë¦¬, í”„ë‘ìŠ¤ (ìœ„ë„: 48.8499, ê²½ë„: 2.3064)\n",
      "3. ì´¬ì˜ ì‹œê¸°: ì—¬ë¦„ìœ¼ë¡œ ë³´ì´ë©°, í•˜ëŠ˜ì´ ë§‘ê³  ì‚¬ëŒë“¤ì´ ë°˜íŒ”ì„ ì…ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„ 6ì›”~8ì›” ì‚¬ì´ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
      "4. í”„â€¦\n",
      "ì£„ì†¡í•˜ì§€ë§Œ ì‚¬ì§„ ì† ì¸ë¬¼ì´ ëˆ„êµ¬ì¸ì§€ëŠ” ì‹ë³„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°°ê²½ ë‹¨ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì •ë§Œ ì œê³µí• ê²Œìš”.\n",
      "\n",
      "1) **ì‚¬ì§„ ì† ì¸ë¬¼**\n",
      "   - ì¸ë¬¼ ì‹ë³„ì€ ê°€ëŠ¥í•˜ë©°, ì‚¬ì§„ ì† ì¸ë¬¼ì˜ êµ¬ì²´ì  ì‹ ì› ì¶”ì •ì„ ì‹œë„ í• ê²Œìš”.\n",
      "2) **ì´¬ì˜ ì¥ì†Œ**\n",
      "   - ë°°ê²½ ë‹¨ì„œë¥¼ ì¢…í•©í•˜ë©´ **UNESCO Headquarters, Paris, France**ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì•„ìš”.\n",
      "   - ëŒ€ëµ ìœ„ì¹˜: 48.860600, 2.337600\n",
      "   - ì§€ë„: https://maps.google.com/?q=48.860600,2.337600\n",
      "3) **ì´¬ì˜ ì‹œê¸°**\n",
      "- (UTC)2025-02-04T09:00:00Z â†’ í•œêµ­ì‹œê°„ **2025-02-04 18:00:00 KST** ë¬´ë µì˜ ì´ë²¤íŠ¸ì™€ ì—°ê´€ë¼ ë³´ì—¬ìš”. (ğŸŸ¢ ì‹ ë¢°ë„ 90%)\n",
      "4) **í”„ë¡œê·¸ë¨/ì´ë²¤íŠ¸**\n",
      "   - **Opening Ceremony of the International Year of Quantum Science and Technology (IYQ) 2025** (Formal Event) ê´€ë ¨ ì¥ë©´ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "   - ê·¼ê±° ìš”ì•½: To celebrate the launch of the International Year of Quantum Science and Technology, recognizing 100 years since the development of quantum mechanics.\n",
      "5) **ì´¬ì˜ ëª©ì **\n",
      "   - To celebrate the launch of the International Year of Quantum Science and Technology, recognizing 100 years since the development of quantum mechanics.\n",
      "\n",
      "â€» ì´ ì¶”ì •ì€ ì‚¬ì§„ì˜ ë°°ê²½, ì˜ìƒ, ê°€ì‹œ í…ìŠ¤íŠ¸ ë“± ê³µê°œ ë‹¨ì„œì—ë§Œ ê·¼ê±°í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# === ë©”ì¸ ì‹¤í–‰: API í‚¤ ì…ë ¥ + ë‹¨ì¼/í´ë” ì„ íƒ ===\n",
    "import os\n",
    "import re\n",
    "from getpass import getpass\n",
    "\n",
    "# 1) í‚¤ ì…ë ¥\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"] =  \"our-openai-api-key-here\"  # OpenAI í‚¤ ì…ë ¥\n",
    "TAVILY_API_KEY = os.environ[\"TAVILY_API_KEY\"] = \"your-tavily-api-key-here\"  # Tavily í‚¤(ì—†ìœ¼ë©´ ê²€ìƒ‰ ìƒëµ)\n",
    "GMAPS_API_KEY = os.environ[\"GMAPS_API_KEY\"] = \"your-google-maps-api-key-here\"  # Google Maps API í‚¤ (ì—†ìœ¼ë©´ ì—­ì§€ì˜¤ì½”ë”© ìƒëµ)\n",
    "\n",
    "# 2) ì‹¤í–‰ ëª¨ë“œ ì„ íƒ: 'file' ë˜ëŠ” 'dir'\n",
    "MODE = \"file\"   # â† 'dir' ë¡œ ë°”ê¾¸ë©´ í´ë” ì „ì²´ ì‹¤í–‰\n",
    "# 2-a) ë‹¨ì¼ íŒŒì¼ ê²½ë¡œ\n",
    "IMAGE_PATH = r\"C:\\Users\\\\ì •í•˜ë¯¼\\\\Desktop\\\\ë•í”½ í…ŒìŠ¤íŠ¸\\\\data\\\\seventeen_eiffel.jpg\"\n",
    "# 2-b) í´ë” ê²½ë¡œ\n",
    "DATA_DIR   = r\"C:\\Users\\ì •í•˜ë¯¼\\Desktop\\ë•í”½ í…ŒìŠ¤íŠ¸\\data\"\n",
    "\n",
    "print(f\"âœ… í‚¤ ì„¤ì • ì™„ë£Œ | Tavily={'ON' if 'TAVILY_API_KEY' in os.environ else 'OFF'}\")\n",
    "\n",
    "if MODE == \"file\":\n",
    "    _ = run_one_chat(IMAGE_PATH, os.environ.get(\"GMAPS_API_KEY\"), os.environ.get(\"OPENAI_API_KEY\"))\n",
    "else:\n",
    "    _ = run_dir_chat(DATA_DIR, os.environ.get(\"GMAPS_API_KEY\"), os.environ.get(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f1ee1a",
   "metadata": {},
   "source": [
    "6ë‹¨ê³„: ì±„íŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install streamlit\n",
    "streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a474ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "\n",
    "# === (ì¤‘ìš”) ë‹¹ì‹ ì˜ ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ë¶ˆëŸ¬ì˜¤ê¸° ===\n",
    "# ê°™ì€ í´ë”ì— ë…¸íŠ¸ë¶ ì½”ë“œë¥¼ 'pipeline.py'ë¡œ ì˜®ê¸°ê±°ë‚˜, ì•„ë˜ì²˜ëŸ¼ ì§ì ‘ import í•˜ì„¸ìš”.\n",
    "# ì—¬ê¸°ì„œëŠ” ë‹¤ìŒ í•¨ìˆ˜ê°€ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤:\n",
    "#   - process_image(image_path, gmaps_api_key, openai_api_key) -> dict\n",
    "#   - format_chat_style(result_dict) -> str\n",
    "\n",
    "\n",
    "st.set_page_config(page_title=\"ë•í”½ ë©€í‹° ë¶„ì„\", page_icon=\"ğŸ“¸\", layout=\"wide\")\n",
    "\n",
    "# --- ì‚¬ì´ë“œë°”: í‚¤ ì…ë ¥/í™˜ê²½ ë³€ìˆ˜ ---\n",
    "st.sidebar.header(\"ğŸ”‘ API ì„¤ì •\")\n",
    "provider = st.sidebar.selectbox(\"Provider\", [\"openai\", \"azure\", \"openai-legacy\"], index=0)\n",
    "openai_key = st.sidebar.text_input(\"OPENAI_API_KEY\", type=\"password\", value=os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
    "gmaps_key  = st.sidebar.text_input(\"GMAPS_API_KEY (ì„ íƒ)\", type=\"password\", value=os.getenv(\"GMAPS_API_KEY\", \"\"))\n",
    "tavily_key = st.sidebar.text_input(\"TAVILY_API_KEY (ì„ íƒ)\", type=\"password\", value=os.getenv(\"TAVILY_API_KEY\", \"\"))\n",
    "\n",
    "if openai_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "if gmaps_key:\n",
    "    os.environ[\"GMAPS_API_KEY\"] = gmaps_key\n",
    "if tavily_key:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = tavily_key\n",
    "\n",
    "use_tavily = bool(tavily_key and tavily_key.strip())\n",
    "st.sidebar.success(f\"Provider={provider} | Tavily={'ON' if use_tavily else 'OFF'}\")\n",
    "\n",
    "st.title(\"ğŸ“¸ ë•í”½ ë©€í‹° ì´ë¯¸ì§€ ë¶„ì„ (Chat ìŠ¤íƒ€ì¼)\")\n",
    "st.caption(\"ì—¬ëŸ¬ ì¥ì„ í•œë²ˆì— ì˜¬ë¦¬ê³ , ê° ì´ë¯¸ì§€ë§ˆë‹¤ ì±— ë¡œê·¸ì²˜ëŸ¼ ê²°ê³¼ë¥¼ ë°›ìœ¼ì„¸ìš”. ì¸ë¬¼ ì‹¤ëª…/ì‹ ì›ì€ ì‹ë³„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "uploaded_files = st.file_uploader(\n",
    "    \"ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”\",\n",
    "    type=[\"jpg\",\"jpeg\",\"png\",\"webp\",\"bmp\",\"tif\",\"tiff\"],\n",
    "    accept_multiple_files=True\n",
    ")\n",
    "\n",
    "col_left, col_right = st.columns([1, 1])\n",
    "\n",
    "if uploaded_files:\n",
    "    with st.spinner(\"ë¶„ì„ ì¤€ë¹„ ì¤‘...\"):\n",
    "        tempdir = Path(tempfile.mkdtemp(prefix=\"dukpick_\"))\n",
    "        paths = []\n",
    "        for f in uploaded_files:\n",
    "            p = tempdir / f.name\n",
    "            with open(p, \"wb\") as out:\n",
    "                out.write(f.read())\n",
    "            paths.append(p)\n",
    "\n",
    "    st.success(f\"{len(paths)}ê°œ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì™„ë£Œ. ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.\")\n",
    "\n",
    "    if st.button(\"ğŸš€ ëª¨ë‘ ë¶„ì„\"):\n",
    "        for i, p in enumerate(paths, 1):\n",
    "            st.markdown(f\"---\\n#### [{i}/{len(paths)}] {p.name}\")\n",
    "            try:\n",
    "                img = Image.open(p)\n",
    "                col_left, col_right = st.columns([1, 1])\n",
    "                with col_left:\n",
    "                    st.image(img, caption=p.name, use_container_width=True)\n",
    "\n",
    "                with col_right:\n",
    "                    res = process_image(str(p), os.getenv(\"GMAPS_API_KEY\"), os.getenv(\"OPENAI_API_KEY\"))\n",
    "                    chat_text = format_chat_style(res)\n",
    "                    # ì±— ë²„ë¸” ëŠë‚Œ\n",
    "                    with st.chat_message(\"assistant\"):\n",
    "                        st.markdown(chat_text)\n",
    "                    with st.expander(\"ì›ë³¸ ê²°ê³¼(JSON) ë³´ê¸°\", expanded=False):\n",
    "                        st.json(res)\n",
    "            except Exception as e:\n",
    "                st.error(f\"ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "else:\n",
    "    st.info(\"ì¢Œì¸¡ì—ì„œ API í‚¤ë¥¼ í™•ì¸í•˜ê³ , ìœ„ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ì¥ ì˜¬ë ¤ì£¼ì„¸ìš”.\")\n",
    "\n",
    "st.caption(\"â€» ê°œì¸ í‚¤ëŠ” ì½”ë“œ/ê¹ƒì— ì ˆëŒ€ ì»¤ë°‹í•˜ì§€ ë§ˆì„¸ìš”. í™˜ê²½ë³€ìˆ˜(.env) ì‚¬ìš© ê¶Œì¥.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920d50e",
   "metadata": {},
   "source": [
    "5ë‹¨ê³„ ê²°ê³¼ PDF ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70413e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“„ data/ ì•„ë˜ ëª¨ë“  ì´ë¯¸ì§€ â†’ (ì™¼ìª½) ì‚¬ì§„ / (ì˜¤ë¥¸ìª½) ê²°ê³¼  PDF ìƒì„±ê¸°\n",
    "# í•„ìš”: pip install reportlab pillow\n",
    "import os, json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from reportlab.lib.pagesizes import A4, landscape\n",
    "from reportlab.lib.units import mm\n",
    "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Image as RLImage, Paragraph, Spacer, PageBreak\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "\n",
    "# âœ… í•œê¸€ í°íŠ¸ ë“±ë¡ (ì—†ìœ¼ë©´ Helveticaë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤)\n",
    "FONT_PATH = os.getenv(\"KR_FONT_PATH\", \"\")  # ì˜ˆ) \"/usr/share/fonts/truetype/noto/NotoSansKR-Regular.otf\"\n",
    "BASE_FONT = \"Helvetica\"\n",
    "if FONT_PATH and os.path.exists(FONT_PATH):\n",
    "    pdfmetrics.registerFont(TTFont(\"KRFont\", FONT_PATH))\n",
    "    BASE_FONT = \"KRFont\"\n",
    "\n",
    "styles = getSampleStyleSheet()\n",
    "styles.add(ParagraphStyle(name=\"BodyKR\", parent=styles[\"Normal\"], fontName=BASE_FONT, fontSize=9, leading=12))\n",
    "styles.add(ParagraphStyle(name=\"TitleKR\", parent=styles[\"Heading2\"], fontName=BASE_FONT))\n",
    "\n",
    "def _fit_image_keep_aspect(path, target_w):\n",
    "    with Image.open(path) as im:\n",
    "        w, h = im.size\n",
    "    target_h = h * (target_w / w)\n",
    "    return target_w, target_h\n",
    "\n",
    "def _nz(x, dash=\"â€”\"):\n",
    "    return x if (x not in (None, \"\", [])) else dash\n",
    "\n",
    "def _trim(text, n=900):\n",
    "    t = (str(text) if text is not None else \"\").strip()\n",
    "    return t if len(t) <= n else t[:n-1] + \"â€¦\"\n",
    "\n",
    "def _right_panel_html(res):\n",
    "    ec = res.get(\"event_card\") or {}\n",
    "    sources = ec.get(\"sources\") or []\n",
    "    src_html = \"<br/>\".join([\n",
    "        f'â€¢ <a href=\"{s.get(\"url\")}\">{_trim(s.get(\"title\") or s.get(\"url\"), 80)}</a>'\n",
    "        for s in sources[:6]\n",
    "    ]) or \"â€”\"\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f'<b>íŒŒì¼:</b> {os.path.basename(res.get(\"image_path\",\"\"))}')\n",
    "    lines.append(f'<b>EXIF ì‹œê°„:</b> {_nz(res.get(\"exif_datetime\"))}')\n",
    "    lat, lon = res.get(\"lat\"), res.get(\"lon\")\n",
    "    lines.append(f'<b>ì¢Œí‘œ:</b> {lat if lat is not None else \"â€”\"}, {lon if lon is not None else \"â€”\"}')\n",
    "    lines.append(f'<b>ì¥ì†Œ:</b> {_nz(res.get(\"place_name\"))}')\n",
    "\n",
    "    if res.get(\"place_description\"):\n",
    "        lines.append(f'<br/><b>ì¥ì†Œ ì†Œê°œ</b><br/>{_trim(res.get(\"place_description\"), 800)}')\n",
    "    if res.get(\"analysis\"):\n",
    "        lines.append(f'<br/><b>ì´ë¯¸ì§€ ë¶„ì„ ìš”ì•½</b><br/>{_trim(res.get(\"analysis\"), 1200)}')\n",
    "\n",
    "    if ec:\n",
    "        lines.append(\"<br/><b>EventCard</b>\")\n",
    "        lines.append(f'â€¢ <b>Method:</b> {_nz(ec.get(\"method\"))}')\n",
    "        if ec.get(\"event_title\"): lines.append(f'â€¢ <b>ì œëª©:</b> {ec.get(\"event_title\")}')\n",
    "        if ec.get(\"event_type\"):  lines.append(f'â€¢ <b>ìœ í˜•:</b> {ec.get(\"event_type\")}')\n",
    "        if ec.get(\"occurred_time_utc\"):\n",
    "            lines.append(f'â€¢ <b>ë°œìƒì‹œê°(UTC):</b> {ec.get(\"occurred_time_utc\")} '\n",
    "                         f'(conf={ec.get(\"occurred_time_confidence\",0):.2f})')\n",
    "        if ec.get(\"location_name\"): lines.append(f'â€¢ <b>ì¥ì†Œëª…:</b> {ec.get(\"location_name\")}')\n",
    "        if ec.get(\"what_happened\"): lines.append(f'â€¢ <b>ë¬´ì—‡:</b> {_trim(ec.get(\"what_happened\"), 600)}')\n",
    "        if ec.get(\"why_summary\"):  lines.append(f'â€¢ <b>ì™œ:</b> {_trim(ec.get(\"why_summary\"), 600)}')\n",
    "        if ec.get(\"who_involved\"): lines.append(f'â€¢ <b>ëˆ„ê°€:</b> {_trim(ec.get(\"who_involved\"), 400)}')\n",
    "        if ec.get(\"key_evidence\"):\n",
    "            kev = \"<br/>\".join([f\" - {_trim(k,120)}\" for k in ec.get(\"key_evidence\")[:6]])\n",
    "            lines.append(f'<b>ê·¼ê±°:</b><br/>{kev}')\n",
    "        lines.append(f'<b>ì¶œì²˜:</b><br/>{src_html}')\n",
    "        if ec.get(\"notes\"): lines.append(f'<br/><b>Notes:</b> {_trim(ec.get(\"notes\"), 400)}')\n",
    "\n",
    "    return \"<br/>\".join(lines)\n",
    "\n",
    "def build_pdf_from_data(data_dir=\"data\", out_pdf=\"outputs/report.pdf\",\n",
    "                        col_ratio=(0.45, 0.55),\n",
    "                        margins=(12*mm, 12*mm, 12*mm, 12*mm)):\n",
    "    \"\"\"\n",
    "    data_dir ì•„ë˜ì˜ ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ì„ ìˆœíšŒí•˜ë©°:\n",
    "      - process_image(...) ì‹¤í–‰\n",
    "      - ê° íŒŒì¼ì„ í•œ í˜ì´ì§€ì— (ì¢Œ)ì´ë¯¸ì§€, (ìš°)ê²°ê³¼ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°°ì¹˜\n",
    "      - PDF ë° ì›ì‹œ ê²°ê³¼ JSON ì €ì¥\n",
    "    â€» API_CFGëŠ” ì•ì„  'API ì„¤ì • ì…€'ì—ì„œ ë§Œë“  dictë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_pdf), exist_ok=True)\n",
    "\n",
    "    # ì´ë¯¸ì§€ ìˆ˜ì§‘\n",
    "    exts = (\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\",\".tif\",\".tiff\")\n",
    "    paths = [p for p in sorted(glob(os.path.join(data_dir, \"**\", \"*\"), recursive=True))\n",
    "             if p.lower().endswith(exts)]\n",
    "    if not paths:\n",
    "        raise RuntimeError(f\"ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {data_dir}\")\n",
    "\n",
    "    # ì „ì²´ ì²˜ë¦¬\n",
    "    results = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            r = process_image(p, API_CFG.get(\"gmaps_api_key\"), API_CFG.get(\"openai_api_key\"))\n",
    "        except Exception as e:\n",
    "            r = {\"image_path\": p, \"error\": str(e), \"event_card\": {\"method\": \"N/A\"}}\n",
    "        results.append(r)\n",
    "\n",
    "    # ì›ì‹œ ê²°ê³¼ ì €ì¥(ì°¸ê³ ìš©)\n",
    "    with open(out_pdf.replace(\".pdf\", \".json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # PDF ì¡°ë¦½\n",
    "    doc = SimpleDocTemplate(\n",
    "        out_pdf,\n",
    "        pagesize=landscape(A4),\n",
    "        leftMargin=margins[0], rightMargin=margins[1],\n",
    "        topMargin=margins[2], bottomMargin=margins[3]\n",
    "    )\n",
    "    PAGE_W, PAGE_H = landscape(A4)\n",
    "    content_w = PAGE_W - (margins[0] + margins[1])\n",
    "    left_w  = content_w * col_ratio[0]\n",
    "    right_w = content_w * col_ratio[1]\n",
    "\n",
    "    story = []\n",
    "    for r in results:\n",
    "        img_path = r[\"image_path\"]\n",
    "        # ì™¼ìª½: ì´ë¯¸ì§€\n",
    "        try:\n",
    "            iw, ih = _fit_image_keep_aspect(img_path, left_w)\n",
    "            img_flow = RLImage(img_path, width=iw, height=ih)\n",
    "        except Exception:\n",
    "            img_flow = Paragraph(\"<b>ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨</b>\", styles[\"BodyKR\"])\n",
    "\n",
    "        # ì˜¤ë¥¸ìª½: ê²°ê³¼ í…ìŠ¤íŠ¸\n",
    "        right_html = _right_panel_html(r)\n",
    "        right_para = Paragraph(right_html, styles[\"BodyKR\"])\n",
    "\n",
    "        table = Table([[img_flow, right_para]], colWidths=[left_w, right_w], hAlign=\"LEFT\")\n",
    "        table.setStyle(TableStyle([\n",
    "            (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
    "            (\"LEFTPADDING\", (0,0), (-1,-1), 6),\n",
    "            (\"RIGHTPADDING\", (0,0), (-1,-1), 6),\n",
    "            (\"TOPPADDING\", (0,0), (-1,-1), 6),\n",
    "            (\"BOTTOMPADDING\", (0,0), (-1,-1), 6),\n",
    "        ]))\n",
    "\n",
    "        title = Paragraph(f\"<b>ë¶„ì„ ë¦¬í¬íŠ¸</b> â€” {os.path.basename(img_path)}\", styles[\"TitleKR\"])\n",
    "        story += [title, Spacer(1, 4*mm), table, PageBreak()]\n",
    "\n",
    "    doc.build(story)\n",
    "    return out_pdf, results\n",
    "\n",
    "#ì‚¬ìš© ì˜ˆ:\n",
    "pdf_path, results = build_pdf_from_data(\"data\", \"outputs/report.pdf\")\n",
    "print(\"PDF ìƒì„±:\", pdf_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
